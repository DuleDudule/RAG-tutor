{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000303d7",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86163934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d0596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00287e1c",
   "metadata": {},
   "source": [
    "Load the parsed contents of the book\n",
    "\n",
    "I used a LLM to generate this json that describes the chapter borders of the book. This makes this ingest method exclusive to this book pdf. Same goes for the page offset. We could make a pipeline that automatically detects the book contents, generates the start and end point of each chapter and a page offset but what happens when we upload a pdf with no contents? Thats the magic of RAG - you can optimize it all you want to your specific use case but it can never be perfect or universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc005be",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../data/processed/contents.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    chapters_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88896e",
   "metadata": {},
   "source": [
    "Set page offset since \"page 1\" of the book is on the 27th page of the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4c2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_OFFSET = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ce2b6",
   "metadata": {},
   "source": [
    "We use the recursive character text splitter that splits entire pages into smaller chunks. It tries to keep the chunks semantic meaning by splitting first by paragraphs, then line breaks and so on until the chunk is smaller that the set chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e2a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_to_book = \"../data/raw/Textbook.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5553c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path_to_book)\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4a0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37e444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chapter in chapters_json:\n",
    "    start_page = chapter[\"start_page\"]\n",
    "    end_page = chapter[\"end_page\"]\n",
    "    \n",
    "    start_idx = start_page + PAGE_OFFSET - 1\n",
    "    \n",
    "    if end_page is not None:\n",
    "        end_idx = end_page + PAGE_OFFSET - 1\n",
    "        chapter_pages = pages[start_idx : end_idx + 1] \n",
    "    else:\n",
    "        chapter_pages = pages[start_idx : ]\n",
    "        \n",
    "    chapter_text = \"\\n\".join([page.page_content for page in chapter_pages])\n",
    "    \n",
    "    if len(chapter_pages) > 0:\n",
    "        chapter_metadata = chapter_pages[0].metadata.copy()\n",
    "    else:\n",
    "        chapter_metadata = {}\n",
    "    \n",
    "    chapter_metadata.pop(\"page\", None)\n",
    "    chapter_metadata.pop(\"page_label\", None)\n",
    "    chapter_metadata.pop(\"subject\", None)\n",
    "    chapter_metadata.pop(\"producer\", None)\n",
    "    chapter_metadata.pop(\"creator\", None)\n",
    "    chapter_metadata.pop(\"creationdate\", None)\n",
    "    chapter_metadata.pop(\"author\", None)\n",
    "    chapter_metadata.pop(\"moddate\", None)\n",
    "    chapter_metadata.pop(\"title\", None)\n",
    "\n",
    "    chapter_metadata[\"chapter_number\"] = chapter[\"chapter_number\"]\n",
    "    chapter_metadata[\"chapter_title\"] = chapter[\"title\"]\n",
    "    \n",
    "    chapter_doc = Document(page_content=chapter_text, metadata=chapter_metadata)\n",
    "    \n",
    "        \n",
    "    chapter_chunks = text_splitter.split_documents([chapter_doc])\n",
    "    \n",
    "    all_chunks.extend(chapter_chunks)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1f1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1251\n",
      "\n",
      "Sample Metadata:\n",
      "{'keywords': '', 'source': '../data/raw/Textbook.pdf', 'total_pages': 746, 'chapter_number': '1', 'chapter_title': 'An Introduction to Data Mining'}\n",
      "\n",
      "Sample chunk content:\n",
      "Chapter 1\n",
      "An Introduction to Data Mining\n",
      "“Education is not the piling on of learning, information, data, facts, skills,\n",
      "or abilities – that’s training or instruction – but is rather making visible\n",
      "what is hidden as a seed.”—Thomas More\n",
      "1.1 Introduction\n",
      "Data mining is the study of collecting, cleaning, processing, analyzing, and gaining useful\n",
      "insights from data. A wide variation exists in terms of the problem domains, applications,\n",
      "formulations, and data representations that are encountered in real applications. Therefore,\n",
      "“data mining” is a broad umbrella term that is used to describe these diﬀerent aspects of\n",
      "data processing.\n",
      "In the modern age, virtually all automated systems generate some form of data either\n",
      "for diagnostic or analysis purposes. This has resulted in a deluge of data, which has been\n",
      "reaching the order of petabytes or exabytes. Some examples of diﬀerent kinds of data are\n",
      "as follows:\n",
      "•World Wide Web:The number of documents on the indexed Web is now on the order\n",
      "of billions, and the invisible Web is much larger. User accesses to such documents\n",
      "create Web access logs at servers and customer behavior proﬁles at commercial sites.\n",
      "Furthermore, the linked structure of the Web is referred to as theWeb graph,w h i c h\n",
      "is itself a kind of data. These diﬀerent types of data are useful in various applications.\n",
      "For example, the Web documents and link structure can be mined to determine asso-\n",
      "ciations between diﬀerent topics on the Web. On the other hand, user access logs can\n",
      "be mined to determine frequent patterns of accesses or unusual patterns of possibly\n",
      "unwarranted behavior.\n",
      "•Financial interactions:Most common transactions of everyday life, such as using an\n",
      "automated teller machine (ATM) card or a credit card, can create data in an auto-\n",
      "mated way. Such transactions can be mined for many useful insights such as fraud or\n",
      "other unusual activity.\n",
      "C. C. Aggarwal,Data Mining: The Textbook, DOI 10.1007/978-3-319-14142-811\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "if len(all_chunks) > 0:\n",
    "    print(\"\\nSample Metadata:\")\n",
    "    print(all_chunks[0].metadata)\n",
    "    print(\"\\nSample chunk content:\")\n",
    "    print(all_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6e477",
   "metadata": {},
   "source": [
    "Since the embedding model only embeds the documents page_content our metadata doesn't contribute in the similarity search later. We could write our rag agent to also filter chunks by metadata but we are going to do something different. We will inject the chunk metadata into page_content to ensure it adds to the semantic meaning of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "312de1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in all_chunks:\n",
    "    ch_num = chunk.metadata.get(\"chapter_number\", \"Unknown\")\n",
    "    ch_title = chunk.metadata.get(\"chapter_title\", \"Unknown Title\")\n",
    "    source_file = chunk.metadata.get(\"source\", \"Unknown Source\")\n",
    "    \n",
    "    metadata_header = (\n",
    "        f\"Chapter {ch_num}: {ch_title}\\n\"\n",
    "        f\"Source: {source_file}\\n\"\n",
    "        f\"----------\\n\"\n",
    "    )\n",
    "\n",
    "    chunk.page_content = metadata_header + chunk.page_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4f7547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PREVIEW OF CHUNK 0 ---\n",
      "Chapter 2: Data Preparation\n",
      "Source: ../data/raw/data_mining_the_textbook.pdf\n",
      "----------\n",
      "ent segments of an image. More recently, the use ofvisual words has become more\n",
      "2.2. FEATURE EXTRACTION AND PORTABILITY 29\n",
      "popular. This is a semantically rich representation that is similar to document data.\n",
      "One challenge in image processing is that the data are generally very high dimen-\n",
      "sional. Thus, feature extraction can be performed at diﬀerent levels, depending on the\n",
      "application at hand.\n",
      "3. Web logs:Web logs are typically represented as text strings in a prespeciﬁed format.\n",
      "Because the ﬁelds in these logs are clearly speciﬁed and separated, it is relatively easy\n",
      "to convert Web access logs into a multidimensional representation of (the relevant)\n",
      "categorical and numeric attributes.\n",
      "4. Network traﬃc:In many intrusion-detection applications, the characteristics of the\n",
      "network packets are used to analyze intrusions or other interesting activity. Depending\n",
      "on the underlying application, a variety of features may be extracted from these\n",
      "packets, such as the number of bytes transferred, the network protocol used, and so\n",
      "on.\n",
      "5. Document data:Document data is often available in raw and unstructured form, and\n",
      "the data may contain rich linguistic relations between diﬀerent entities. One approach\n",
      "is to remove stop words, stem the data, and use a bag-of-words representation. Other\n",
      "methods useentity extractionto determine linguistic relationships.\n",
      "Named-entity recognition is an important subtask of information extraction. This\n",
      "approach locates and classiﬁes atomic elements in text into predeﬁned expressions\n",
      "of names of persons, organizations, locations, actions, numeric quantities, and so on.\n",
      "Clearly, the ability to identify such atomic elements is very useful because they can be\n",
      "used to understand the structure of sentences and complex events. Such an approach\n",
      "can also be used to populate a more conventional database of relational elements or\n",
      "as a sequence of atomic entities, which is more easily analyzed. For example, consider\n",
      "the following sentence:\n"
     ]
    }
   ],
   "source": [
    "print(\"--- PREVIEW OF CHUNK 0 ---\")\n",
    "print(all_chunks[50].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dfcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-tutor-py3.12 (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
