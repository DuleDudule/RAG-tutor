{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model Demonstration\n",
    "\n",
    "This notebook demonstrates the core concept behind Retrieval Augmented Generation - **Vector Embeddings**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcadf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from src.util.env_check import get_embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e36d29",
   "metadata": {},
   "source": [
    "Loading the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab44f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: model='qwen3-embedding:0.6b' base_url='http://localhost:11434' client_kwargs={} mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None\n"
     ]
    }
   ],
   "source": [
    "embedding_model = get_embed_model()\n",
    "print(f\"Loaded embedding model: {embedding_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f99fc",
   "metadata": {},
   "source": [
    "### Simple Sentence Demonstration\n",
    "\n",
    "Embedding models are machine learning models used to capture meaning behind data. They are often based on transformer architectures now but they started off as simple shallow neural networks. They convert complex unstructured data (text, images, or audio) into dense numerical vectors (embeddings). These vectors capture semantic meaning, allowing computers to identify relationships, context, and similarities, essential for applications like semantic search, recommendation systems, and Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "To demonstrate embedding models on a small example lets embed 2 sets of sentences:\n",
    "1.  Animals/Nature\n",
    "2.  Computer Science/Data Mining\n",
    "\n",
    "We expect the model to group semantically similar sentences together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8bcbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (8, 1024)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    #Animals\n",
    "    \"The quick brown fox jumps over the dog.\",\n",
    "    \"The neigbourhood dog barks all night.\",\n",
    "    \"My cat likes to chase dogs in the yard.\",\n",
    "    \"Cheetas are very fast.\",\n",
    "    \n",
    "    #Data Mining\n",
    "    \"Data mining involves discovering patterns in large datasets.\",\n",
    "    \"Neural networks are a subset of machine learning algorithms.\",\n",
    "    \"Clustering algorithms group similar data points together.\",\n",
    "    \"Principal component analysis reduces the dimensionality of data.\"\n",
    "]\n",
    "\n",
    "\n",
    "# For color coding\n",
    "labels = [\"Animal\",\"Animal\",\"Animal\",\"Animal\",\"Tech\",\"Tech\",\"Tech\",\"Tech\"]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_model.embed_documents(sentences)\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "print(f\"Generated embeddings shape: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4145a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02527663, -0.04816985, -0.00368367, ...,  0.03945439,\n",
       "       -0.00913975, -0.00020009])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dfc99",
   "metadata": {},
   "source": [
    "### Visualization (PCA)\n",
    "Embeddings are high-dimensional vectors. Lets use Principal Component Analysis (PCA) to reduce them to 3 dimensions so we can plot them.\n",
    "\n",
    "You can interact with the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb33fd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "The quick brown fox jumps over the dog."
          ],
          [
           "The neigbourhood dog barks all night."
          ],
          [
           "My cat likes to chase dogs in the yard."
          ],
          [
           "Cheetas are very fast."
          ]
         ],
         "hovertemplate": "category=Animal<br>sentence=%{customdata[0]}<extra></extra>",
         "legendgroup": "Animal",
         "marker": {
          "color": "green",
          "size": 5,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Animal",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "X60QlpYz2T8IJwZULzDcPxaipEr2F+A/EfPR9Qlr2T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "O40Pt6eB1b8irKg+/0jgP9ikDTvqvcQ/ZVIZb3Js2b8=",
          "dtype": "f8"
         },
         "z": {
          "bdata": "i6FQ8wiOtz9Otzfveb7CP5inWoQDlJ0/yu+7qXpqzr8=",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Data mining involves discovering patterns in large datasets."
          ],
          [
           "Neural networks are a subset of machine learning algorithms."
          ],
          [
           "Clustering algorithms group similar data points together."
          ],
          [
           "Principal component analysis reduces the dimensionality of data."
          ]
         ],
         "hovertemplate": "category=Tech<br>sentence=%{customdata[0]}<extra></extra>",
         "legendgroup": "Tech",
         "marker": {
          "color": "blue",
          "size": 5,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Tech",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "1hqtOcFE3r++Kefoda3Yv9iILqksLdq/OD5vqVjf3b8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "JoChEvjtoD9r0ihnosvIP90hoA2wiaG/HsDHUWeqwL8=",
          "dtype": "f8"
         },
         "z": {
          "bdata": "fvUms+AUtT/QYX7GxHPhv+xg0agiO8Y/Kp/y8/2d0D8=",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "category"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 40
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         },
         "zaxis": {
          "title": {
           "text": "z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Sentence Embeddings (interactive plot)"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_pca = PCA(n_components=3)\n",
    "reduced_embeddings = simple_pca.fit_transform(embeddings_array)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': reduced_embeddings[:, 0],\n",
    "    'y': reduced_embeddings[:, 1],\n",
    "    'z': reduced_embeddings[:, 2],\n",
    "    'sentence': sentences,\n",
    "    'category': labels\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df, \n",
    "    x='x', y='y', z='z',\n",
    "    color='category',\n",
    "    hover_data={'x': False, 'y': False, 'z': False, 'sentence': True},\n",
    "    title=\"3D Sentence Embeddings (interactive plot)\",\n",
    "    color_discrete_map={'Animal': 'green', 'Tech': 'blue'}\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=40))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a99be9",
   "metadata": {},
   "source": [
    "### Textbook Analysis\n",
    "\n",
    "Now we apply this to the Textbook:\n",
    "1.  Load the Textbook.\n",
    "2.  Use the `contents.json` map to split it into chapters.\n",
    "3.  Embed a sample of text chunks from each chapter.\n",
    "4.  Visualize if chapters cluster separately in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e2e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 746 pages from the textbook.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "json_path = \"../data/processed/contents.json\"\n",
    "pdf_path = \"../data/raw/Textbook.pdf\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    chapters_json = json.load(f)\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(pages)} pages from the textbook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f75f5",
   "metadata": {},
   "source": [
    "### Processing Chapters\n",
    "Using the logic from `advanced_ingest.py` to split the book by chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b87fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks prepared for embedding: 90\n"
     ]
    }
   ],
   "source": [
    "PAGE_OFFSET = 26 \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "chapter_chunks = []\n",
    "chapter_labels = []\n",
    "\n",
    "# Processing only the first 6 chapters for clarity in the plot\n",
    "target_chapters = chapters_json[:6]\n",
    "\n",
    "for chapter in target_chapters:\n",
    "    start_page = chapter[\"start_page\"]\n",
    "    end_page = chapter[\"end_page\"]\n",
    "    ch_title = chapter[\"title\"]\n",
    "    ch_num = chapter[\"chapter_number\"]\n",
    "    \n",
    "    start_idx = start_page + PAGE_OFFSET - 1\n",
    "    if end_page is not None:\n",
    "        end_idx = end_page + PAGE_OFFSET - 1\n",
    "        current_pages = pages[start_idx : end_idx + 1]\n",
    "    else:\n",
    "        current_pages = pages[start_idx : ]\n",
    "        \n",
    "    full_text = \"\\n\".join([p.page_content for p in current_pages])\n",
    "    \n",
    "    doc = Document(page_content=full_text, metadata={\"chapter\": f\"Ch {ch_num}\"})\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Take some chunks out of each chapter\n",
    "    sample_chunks = chunks[:15]\n",
    "    \n",
    "    for chunk in sample_chunks:\n",
    "        contextualized_text = f\"Chapter {ch_num}: {ch_title}\\n{chunk.page_content}\"\n",
    "        chapter_chunks.append(contextualized_text)\n",
    "        chapter_labels.append(f\"Ch {ch_num}\")\n",
    "\n",
    "print(f\"Total chunks prepared for embedding: {len(chapter_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f453f",
   "metadata": {},
   "source": [
    "Generate Chapter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0467d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_embeddings = embedding_model.embed_documents(chapter_chunks)\n",
    "book_embeddings_array = np.array(book_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b1e41",
   "metadata": {},
   "source": [
    "### 3D Visualization of chapters\n",
    "We plot the chunks in 3D space, coloring them by chapter. We should see chunks from the same chapters cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fb8fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ],
          [
           "Ch 1"
          ]
         ],
         "hovertemplate": "chapter=%{customdata[0]}<extra></extra>",
         "legendgroup": "Ch 1",
         "marker": {
          "color": "#636EFA",
          "line": {
           "width": 0
          },
          "opacity": 0.8,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 1",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "rBdxnNRqxD9SQQWrzhiIP9E2LpV7org/SGJpcQydzD+r6OQxE+fIP4NlbRZJpbU/ncDDXDlH0T9cSgv4kmnGP13oYdziLtE/hciTNO7v0T9dzxgBbzLQPwtmiw7CNsU/OTPN4RuKwT/ilSdkKHzKP7JGcGyPKsY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "830kl6Fq3b9ip+PKC+bVv7hQ5/rAWte/K7VCTREz4b9f5T6uTGTdvwf6BQzzadK/raSc3QMf1L80mtJTgpHevwPfkjwvNeC/Qd7131IW37/w7yAbnCrXvyYplUJEPdu/LSfDjc3Y1b9Xb2gHQU/Sv2ooiG8U6NG/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "APLlAAaqsj9VDNHdAHitP5qRfvvScqQ/iFc7z/ESij+qcona7Eq7P5ryBwDYj8A/QfGhvFzSzD/urF3/Z/mtPwHNujQZJqG/MtbmVYspwr+vgVqQWsy1v/LZFr002Ki/M5hh3IQmnz/6EMRKKGufP/OIbDFtQ8W/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ],
          [
           "Ch 2"
          ]
         ],
         "hovertemplate": "chapter=%{customdata[0]}<extra></extra>",
         "legendgroup": "Ch 2",
         "marker": {
          "color": "#EF553B",
          "line": {
           "width": 0
          },
          "opacity": 0.8,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 2",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "alXbtLfV0j/SmQQ0y5XZP+gvD1qyCNM/4YF2RTeQzj8x/wDHEnvYP1MMRSRkOdU/RgZC52cD0T+VI482rufMP7xeIDuN+8E/EzbtxzKv0T+QRAdS6QDTP8ATSTEatss/qrUZK/3wxj+BEEguihPLP1A6jdcR/9I/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ZEeTq/Xx2L/1KQWAcy7Wv10dbCpPNtq/lLptKbqrw7+jUF4MfjLZv7hlmKMXgNK/O9IID0cszL+PhuXwRdDRv2q8F0crStC/pBkJTpxMw79ITUc6wD/Xv+lTgFIwoc6/hctKSZsjrr8f49MiYFG5P4JfvDVC1LW/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "8CVceVo7rb81hmOwmBPQvyYMLipW+Fa/KcaS7buvzb+A/leRfZrOv9JrLxB/E8G/XGOzRhupsr9zXNTcidPCv5kpOOLP37C/MHbADD1J0L86mF32SoiyPzxmK/5HWLE/n7INXVuKlT/SBzvGkOGwP33GJYFoup2/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ],
          [
           "Ch 3"
          ]
         ],
         "hovertemplate": "chapter=%{customdata[0]}<extra></extra>",
         "legendgroup": "Ch 3",
         "marker": {
          "color": "#00CC96",
          "line": {
           "width": 0
          },
          "opacity": 0.8,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 3",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "Pn+GgbYZzj+4+ORosMLQPy1u4AMwQNQ/xzMNwGOo0j/Nh2hVMBfXP1oQT7y4KdM/3d+b8rzO1z8CGoQCJJXUP01jfKie1dM/PtctKUAA0D8WPrKx/ZfNP5NHZv7oGtQ/zSK65f2e1j/7/dy+Q9HWP2nOQ36rRNI/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "2TEbscrfyT/YEGwGNaLKP0HupWFMW8M/HVQ3TgO9xz+OPNU/yhvOP0K1D6d8l9A/1yZ3LLQN1D8x5gQATafZP2Nr54wPnNM/AYX7QFT+0j+9C/mKCrDNP45wBqQZK88/pNcgG8Ly2j+5dNzvexfVP2TbOAdxOdo/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "Q9VlG4Nx0z9hnKSnwrzUPwbUNXca4dM/TlbTHA6d0z+xZ+v3suPbP5MqVMAtXtg/EViBLTbo1T8LQhwfhgrJP8nHY/dfVdI/wHG69WwB2D/wHh03S0fZPzloqBR6+dM/RV9ot5UUwD+596YT7Z/QP5wH/EoNX9c/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ],
          [
           "Ch 4"
          ]
         ],
         "hovertemplate": "chapter=%{customdata[0]}<extra></extra>",
         "legendgroup": "Ch 4",
         "marker": {
          "color": "#AB63FA",
          "line": {
           "width": 0
          },
          "opacity": 0.8,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 4",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "ihm5iAlX2L8eYQVHzq7Uv4Hg1TWI09i/oQNdWqnB2L9r3UQ06UbhvwVL6aVjKuG/Z/Fv5Jcd3b+t4A68mnXTv0goPcszieG/ac8WcpkH4r8fbBkItpPiv28o6PM4z+K/SPioiVPP4L9ubR3CIbzgv7/Ct3e9+tu/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "n8mBmC3nw7+JEZS8cwKSv2sQQs9nyri/kMWQPr2gvL/4RKC4le6sv2/oKa7TTLq/Qi8wQ6B2ub/4xrQm8Aenv93CraQaFLC/1vMrS6KJfL9iv2DnwpGJP/qqizLPZrW/M8B/wqMuoT/tWr75gAOoP5t0Uejegow/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "aCSNDOYQcL9kwE0w3YSvPxD0FVkdhXO/3QIXwp+Kwz8mP1LWiDmqPw//kSWqiaM/4pIKlABauT9EtqXVx4HEP6WJhUDKXro/2oG/sLGSsj81A50/YjiXP20gZV6RfKs/R1KbhhAWqj8Tkzw+PVWQPw0idPjWHaI/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ],
          [
           "Ch 5"
          ]
         ],
         "hovertemplate": "chapter=%{customdata[0]}<extra></extra>",
         "legendgroup": "Ch 5",
         "marker": {
          "color": "#FFA15A",
          "line": {
           "width": 0
          },
          "opacity": 0.8,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 5",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "4zxwZsGp27+90EbnfyPZv+4DOXHd7tW/XQ3ZbAHS37/d6+YrZ9fhv+bRI2vK6N+/U1SD7Rca4r+i5URXG87hv6U5le3cQOG/Gszng1Ss278ldk2tZZTcv74JwGj1Q9y/R9OghJM+3L/DXKpkxCXfv5jdbFxSSNe/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "oo+Q/40dpL9wjoRc1ql6v2svR8YD7Kw/jsvWvrZKnj/2MjmPeZ+pP8G9x8O98bE/cb7WiA+MmL/3JeGvX0ymP8Z6xEM3BbE/k/w6zV/Jtj+Qirv8Bfu1P74JqUgVy8Q/M9bgy5teqz8dVPXWQZK2P75a3PXztMo/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "C8DSfq65lT9x/NeObHe1v97OUDTm3cC/6h2CBA47qb+beCGPgkKuP930jz/Bk30/6KYPCegKpL9MGtm4+8qov2O163CJV62/MLbz+uLuej+VOwfglL6qP8ogy5pScqe/GkBkLuU8br9rVCI3z4p9v0nf6ngBN3W/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ],
          [
           "Ch 6"
          ]
         ],
         "hovertemplate": "chapter=%{customdata[0]}<extra></extra>",
         "legendgroup": "Ch 6",
         "marker": {
          "color": "#19D3F3",
          "line": {
           "width": 0
          },
          "opacity": 0.8,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 6",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "rtIXgCp6yj+xon6Z7W3JP9aeAEIlA78/nl5OcOJfyT/XloCOkVrOP65n1Sn/E8w/NO/HPMspyj9VQDWVuovPP737GPj6J7c/stVmax70xT9sg0xVBQDAPyYNOeNRpcY/jzclTUg8yj9YEtH6lUPFP895ks8Ls8g/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "4Zb14vbm0T+0HNWDUjzSP/D1YvdMl9c/Ku0zFD0d1D872sEYRBPTP386PfQOUNo/Wz28yZoo1z/muPtUPXnVP60Es6gdWNI/tqcugWUF1D+ZIgtaP6LVP+ceGxn39Ng/bbaNSgYX1T+Ybk7oqq7XP5JpxANlutg/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "Iy7qb+ZG4b91HRioQrrQv8Tbt0ZJDr6/7soM5dWHzb+O7490C1eIv4nNjJTOING/9PMT5VcJ0L/DCid5WOLDvwTyIsHM2tq/I4c+tMPhaL/Z0ymCVnvZvzYhMJLrMt6/whjMdFmv3r9LvNLJmnLVv89nRqWtFdm/",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Chapter"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 40
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "PC1"
          }
         },
         "yaxis": {
          "title": {
           "text": "PC2"
          }
         },
         "zaxis": {
          "title": {
           "text": "PC3"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Semantic Clusters of Textbook Chapters"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduced_book = pca.fit_transform(book_embeddings_array)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': reduced_book[:, 0],\n",
    "    'y': reduced_book[:, 1],\n",
    "    'z': reduced_book[:, 2],\n",
    "    'chapter': chapter_labels,\n",
    "    'text': chapter_chunks\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df, \n",
    "    x='x', y='y', z='z',\n",
    "    color='chapter',\n",
    "    hover_data={'x': False, 'y': False, 'z': False, 'chapter': True},\n",
    "    title=\"Semantic Clusters of Textbook Chapters\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Plotly\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=4, opacity=0.8, line=dict(width=0))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    "    legend_title_text='Chapter',\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddfa02",
   "metadata": {},
   "source": [
    "The plot above demonstrates that semantically related text clusters together in vector space.\n",
    "\n",
    "- Chunks from the same chapter appear close to each other.\n",
    "- Chunks from different topics are separated.\n",
    "\n",
    "\n",
    "### How This Applies to RAG (Retrieval Augmented Generation)\n",
    "\n",
    "When a user asks a question, a RAG system performs the following steps:\n",
    "1. Embed the query: It takes the users raw text question and passes it through the exact same embedding model used for the database.\n",
    "2. Calculate similarity: It compares the querys vector against every chunks vector in the database to find the closest matches. \n",
    "3. Retrieve and generate: It takes the top K most similar chunks and feeds them to an LLM as context to formulate an accurate answer.\n",
    "\n",
    "**Cosine Similarity**\n",
    "\n",
    "To find the closest vectors, the most common metric used is cosine similarity. Instead of measuring the physical distance between two points, it measures the angle between two vectors. This makes it highly effective for text, as it focuses on the orientation (semantic meaning) rather than the magnitude (length of the document).\n",
    "\n",
    "The formula for Cosine Similarity between vectors $A$ and $B$ is:\n",
    "$$\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "A score of **1** means the vectors point in the exact same direction (highly similar), **0** means they are unrelated, and **-1** means they are completely opposite.\n",
    "\n",
    "### Simple demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa815abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00352722,  0.00419386, -0.00748914, ...,  0.04388433,\n",
       "       -0.01641025, -0.02402433])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "user_query = \"What wild cat is the quickest?\"\n",
    "\n",
    "query_embedding = embedding_model.embed_documents([user_query])[0]\n",
    "query_vector = np.array(query_embedding)\n",
    "query_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075d7cc",
   "metadata": {},
   "source": [
    "This is essentially what happens when we run similarity search on a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164f1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: 'What wild cat is the quickest?'\n",
      "\n",
      "--------------------------------------------------\n",
      "Top Database Match (Similarity Score: 0.6797):\n",
      "'Cheetas are very fast.'\n",
      "--------------------------------------------------\n",
      "\n",
      "All scores ranked:\n",
      "[0.6797] Cheetas are very fast.\n",
      "[0.5362] My cat likes to chase dogs in the yard.\n",
      "[0.5324] The quick brown fox jumps over the dog.\n",
      "[0.3996] The neigbourhood dog barks all night.\n",
      "[0.2979] Principal component analysis reduces the dimensionality of data.\n",
      "[0.2929] Data mining involves discovering patterns in large datasets.\n",
      "[0.2901] Clustering algorithms group similar data points together.\n",
      "[0.2507] Neural networks are a subset of machine learning algorithms.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarities = []\n",
    "for sentence_vector in embeddings_array:\n",
    "    sim = cosine_similarity(query_vector, sentence_vector)\n",
    "    similarities.append(sim)\n",
    "\n",
    "best_match_idx = np.argmax(similarities)\n",
    "best_score = similarities[best_match_idx]\n",
    "best_sentence = sentences[best_match_idx]\n",
    "\n",
    "print(f\"User Query: '{user_query}'\\n\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Top Database Match (Similarity Score: {best_score:.4f}):\")\n",
    "print(f\"'{best_sentence}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nAll scores ranked:\")\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "for idx in ranked_indices:\n",
    "    print(f\"[{similarities[idx]:.4f}] {sentences[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d7b70",
   "metadata": {},
   "source": [
    "Visualizing this similarity and how it applies to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b981026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "The quick brown fox jumps over the dog.",
           5
          ],
          [
           "The neigbourhood dog barks all night.",
           5
          ],
          [
           "My cat likes to chase dogs in the yard.",
           5
          ],
          [
           "Cheetas are very fast.",
           5
          ]
         ],
         "hovertemplate": "category=Animal<br>sentence=%{customdata[0]}<extra></extra>",
         "legendgroup": "Animal",
         "marker": {
          "color": "green",
          "size": {
           "bdata": "BQUFBQ==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.1,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Animal",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "YZLRoO1p2T9K4GtE4XbcP2HKFAxLHeA/Mf6fqkFt2T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "0HmV94k61r9sSDDcwz/gPyPXDIptHsU/uMRGvdrb2L8=",
          "dtype": "f8"
         },
         "z": {
          "bdata": "eoydW0dBuD9KRVkf2OjCP2JWn8lR3pY/BILKShtvzr8=",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Data mining involves discovering patterns in large datasets.",
           5
          ],
          [
           "Neural networks are a subset of machine learning algorithms.",
           5
          ],
          [
           "Clustering algorithms group similar data points together.",
           5
          ],
          [
           "Principal component analysis reduces the dimensionality of data.",
           5
          ]
         ],
         "hovertemplate": "category=Tech<br>sentence=%{customdata[0]}<extra></extra>",
         "legendgroup": "Tech",
         "marker": {
          "color": "blue",
          "size": {
           "bdata": "BQUFBQ==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.1,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Tech",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "Ubp3f+d/3r+dkh+1UuHYv3uaFQsG9tm/Kx5aaGYx3r8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "6nZf8lXooz/fx0fuAA3IP6JVusMqjqK/zosGSz9UwL8=",
          "dtype": "f8"
         },
         "z": {
          "bdata": "hmAZOTPlsT8Rl1XR3UHhvzuXvgDArsc/RdA8dvl30D8=",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "What wild cat is the quickest?",
           10
          ]
         ],
         "hovertemplate": "category=User Query<br>sentence=%{customdata[0]}<extra></extra>",
         "legendgroup": "User Query",
         "marker": {
          "color": "red",
          "size": {
           "bdata": "Cg==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.1,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "User Query",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "r6bQXuv41D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "UvVP33LmyL8=",
          "dtype": "f8"
         },
         "z": {
          "bdata": "0GdVUGK5tL8=",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "legend": {
         "itemsizing": "constant",
         "title": {
          "text": "category"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 40
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         },
         "zaxis": {
          "title": {
           "text": "z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Visualizing the user query in vector space"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduced_query = simple_pca.transform([query_vector])\n",
    "\n",
    "all_reduced_embeddings = np.vstack([reduced_embeddings, reduced_query])\n",
    "all_sentences = sentences + [user_query]\n",
    "all_labels = labels + [\"User Query\"]\n",
    "\n",
    "sizes = [5] * len(sentences) + [10]\n",
    "\n",
    "df_query = pd.DataFrame({\n",
    "    'x': all_reduced_embeddings[:, 0],\n",
    "    'y': all_reduced_embeddings[:, 1],\n",
    "    'z': all_reduced_embeddings[:, 2],\n",
    "    'sentence': all_sentences,\n",
    "    'category': all_labels,\n",
    "    'size': sizes\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_query, \n",
    "    x='x', y='y', z='z',\n",
    "    color='category',\n",
    "    size='size',\n",
    "    size_max=10,\n",
    "    hover_data={'x': False, 'y': False, 'z': False, 'sentence': True, 'size': False},\n",
    "    title=\"Visualizing the user query in vector space\",\n",
    "    color_discrete_map={'Animal': 'green', 'Tech': 'blue', 'User Query': 'red'}\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=40))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7a5586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Chapter 1: An Introduction to Data Mining\nChapter 1\nAn Introduction to Data Mining\nEducation is not the piling on of learning, information, data, facts, skills,\nor abilities  thats training or instruction  but is rather making visible\nwhat is hidden as a seed.Thomas More\n1.1 Introduction\nData mining is the study of collecting, cleaning, processing, analyzing, and gaining useful\ninsights from data. A wide variation exists in terms of the problem domains, applications,\nformulations, and data representations that are encountered in real applications. Therefore,\ndata mining is a broad umbrella term that is used to describe these dierent aspects of\ndata processing.\nIn the modern age, virtually all automated systems generate some form of data either\nfor diagnostic or analysis purposes. This has resulted in a deluge of data, which has been\nreaching the order of petabytes or exabytes. Some examples of dierent kinds of data are\nas follows:\nWorld Wide Web:The number of documents on the indexed Web is now on the order",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nas follows:\nWorld Wide Web:The number of documents on the indexed Web is now on the order\nof billions, and the invisible Web is much larger. User accesses to such documents\ncreate Web access logs at servers and customer behavior proles at commercial sites.\nFurthermore, the linked structure of the Web is referred to as theWeb graph,w h i c h\nis itself a kind of data. These dierent types of data are useful in various applications.\nFor example, the Web documents and link structure can be mined to determine asso-\nciations between dierent topics on the Web. On the other hand, user access logs can\nbe mined to determine frequent patterns of accesses or unusual patterns of possibly\nunwarranted behavior.\nFinancial interactions:Most common transactions of everyday life, such as using an\nautomated teller machine (ATM) card or a credit card, can create data in an auto-\nmated way. Such transactions can be mined for many useful insights such as fraud or\nother unusual activity.",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nother unusual activity.\nC. C. Aggarwal,Data Mining: The Textbook, DOI 10.1007/978-3-319-14142-811\nc Springer International Publishing Switzerland 2015\n2 CHAPTER 1. AN INTRODUCTION TO DATA MINING\nUser interactions:Many forms of user interactions create large volumes of data. For\nexample, the use of a telephone typically creates a record at the telecommunication\ncompany with details about the duration and destination of the call. Many phone\ncompanies routinely analyze such data to determine relevant patterns of behavior\nthat can be used to make decisions about network capacity, promotions, pricing, or\ncustomer targeting.\nSensor technologies and the Internet of Things:A recent trend is the development\nof low-cost wearable sensors, smartphones, and other smart devices that can commu-\nnicate with one another. By one estimate, the number of such devices exceeded the\nnumber of people on the planet in 2008 [30]. The implications of such massive data",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nnumber of people on the planet in 2008 [30]. The implications of such massive data\ncollection are signicant for mining algorithms.\nThe deluge of data is a direct result of advances in technology and the computerization of\nevery aspect of modern life. It is, therefore, natural to examine whether one can extract\nconciseandpossibly actionableinsightsfromtheavailabledataforapplication-specicgoals.\nThisiswherethetaskofdataminingcomesin.Therawdatamaybearbitrary,unstructured,\nor even in a format that is not immediately suitable for automated processing. For example,\nmanually collected data may be drawn from heterogeneous sources in dierent formats and\nyet somehow needs to be processed by an automated computer program to gain insights.\nTo address this issue, data mining analysts use a pipeline of processing, where the raw\ndata are collected, cleaned, and transformed into a standardized format. The data may be",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\ndata are collected, cleaned, and transformed into a standardized format. The data may be\nstored in a commercial database system and nally processed for insights with the use of\nanalytical methods. In fact, while data mining often conjures up the notion of analytical\nalgorithms, the reality is that the vast majority of work is related to the data preparation\nportion of the process. This pipeline of processing is conceptually similar to that of an actual\nmining process from a mineral ore to the rened end product. The term mining derives\nits roots from this analogy.\nFrom an analytical perspective, data mining is challenging because of the wide disparity\nin the problems and data types that are encountered. For example, a commercial product\nrecommendation problem is very dierent from an intrusion-detection application, even at\nthe level of the input data format or the problem denition. Even within related classes",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nthe level of the input data format or the problem denition. Even within related classes\nof problems, the dierences are quite signicant. For example, a product recommendation\nproblem in a multidimensional database is very dierent from a social recommendation\nproblem due to the dierences in the underlying data type. Nevertheless, in spite of these\ndierences, data mining applications are often closely connected to one of four super-\nproblems in data mining: association pattern mining, clustering, classication, and outlier\ndetection. These problems are so important because they are used as building blocks in a\nmajority of the applications in some indirect form or the other. This is a useful abstraction\nbecause it helps us conceptualize and structure the eld of data mining more eectively.\nThe data may have dierent formats ortypes. The type may be quantitative (e.g., age),\ncategorical (e.g., ethnicity), text, spatial, temporal, or graph-oriented. Although the most",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\ncategorical (e.g., ethnicity), text, spatial, temporal, or graph-oriented. Although the most\ncommonformofdataismultidimensional, anincreasingproportionbelongstomorecomplex\ndata types. While there is a conceptual portability of algorithms between many data types\nat a very high level, this is not the case from a practical perspective. The reality is that\nthe precise data type may aect the behavior of a particular algorithm signicantly. As a\nresult, one may need to design rened variations of the basic approach for multidimensional\ndata, so that it can be used eectively for a dierent data type. Therefore, this book will\ndedicate dierent chapters to the various data types to provide a better understanding of\nhow the processing methods are aected by the underlying data type.\n1.2. THE DATA MINING PROCESS 3\nA major challenge has been created in recent years due to increasing data volumes. The\nprevalence of continuously collected data has led to an increasing interest in the eld ofdata",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nprevalence of continuously collected data has led to an increasing interest in the eld ofdata\nstreams. For example, Internet trac generates large streams that cannot even be stored\neectively unless signicant resources are spent on storage. This leads to unique challenges\nfrom the perspective of processing and analysis. In cases where it is not possible to explicitly\nstore the data, all the processing needs to be performed in real time.\nThis chapter will provide a broad overview of the dierent technologies involved in pre-\nprocessing and analyzing dierent types of data. The goal is to study data mining from the\nperspective of dierent problem abstractions and data types that are frequently encoun-\ntered. Many important applications can be converted into these abstractions.\nThis chapter is organized as follows. Section1.2 discusses the data mining process with\nparticular attention paid to the data preprocessing phase in this section. Dierent data",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nparticular attention paid to the data preprocessing phase in this section. Dierent data\ntypes and their formal denition are discussed in Sect.1.3. The major problems in data\nmining are discussed in Sect.1.4 at a very high level. The impact of data type on problem\ndenitions is also addressed in this section. Scalability issues are addressed in Sect.1.5.I n\nSect. 1.6, a few examples of applications are provided. Section1.7 gives a summary.\n1.2 The Data Mining Process\nAs discussed earlier, the data mining process is a pipeline containing many phases such as\ndata cleaning, feature extraction, and algorithmic design. In this section, we will study these\ndierent phases. The workow of a typical data mining application contains the following\nphases:\n1. Data collection:Data collection may require the use of specialized hardware such as a\nsensor network, manual labor such as the collection of user surveys, or software tools",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nsensor network, manual labor such as the collection of user surveys, or software tools\nsuch as a Web document crawling engine to collect documents. While this stage is\nhighly application-specic and often outside the realm of the data mining analyst,\nit is critically important because good choices at this stage may signicantly impact\nthe data mining process. After the collection phase, the data are often stored in a\ndatabase, or, more generally, adata warehousefor processing.\n2. Feature extraction and data cleaning:When the data are collected, they are often not\nin a form that is suitable for processing. For example, the data may be encoded in\ncomplex logs or free-form documents. In many cases, dierent types of data may be\narbitrarily mixed together in a free-form document. To make the data suitable for\nprocessing, it is essential to transform them into a format that is friendly to data\nmining algorithms, such as multidimensional, time series, or semistructured format.",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nmining algorithms, such as multidimensional, time series, or semistructured format.\nThe multidimensional format is the most common one, in which dierentelds of the\ndata correspond to the dierent measured properties that are referred to asfeatures,\nattributes,o r dimensions. It is crucial to extract relevant features for the mining\nprocess. The feature extraction phase is often performed in parallel with data cleaning,\nwhere missing and erroneous parts of the data are either estimated or corrected. In\nmany cases, the data may be extracted from multiple sources and need to beintegrated\ninto a unied format for processing. The nal result of this procedure is a nicely\nstructured data set, which can be eectively used by a computer program. After the\nfeature extraction phase, the data may again be stored in a database for processing.\n3. Analytical processing and algorithms:The nal part of the mining process is to design",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\n3. Analytical processing and algorithms:The nal part of the mining process is to design\neective analytical methods from the processed data. In many cases, it may not be\n4 CHAPTER 1. AN INTRODUCTION TO DATA MINING\nDATA\nPREPROCESSING ANALYTICAL PROCESSINGDATA\nCOLLECTION\nPREPROCESSING\nFEATURE\nEXTRACTION\nANALYTICAL PROCESSING OUTPUT\nFOR\nANALYST\nCLEANING\nAND\nINTEGRATION\nBUILDING\nBLOCK 1\nBUILDING\nBLOCK 2\nFEEDBACK (OPTIONAL)\nFEEDBACK (OPTIONAL)\n(\nFigure 1.1: The data processing pipeline\npossible to directly use a standard data mining problem, such as the four superprob-\nlems discussed earlier, for the application at hand. However, these four problems have\nsuch wide coverage thatmany applications can be broken up into components that\nuse these dierent building blocks. This book will provide examples of this process.\nThe overall data mining process is illustrated in Fig.1.1. Note that the analytical block in",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nThe overall data mining process is illustrated in Fig.1.1. Note that the analytical block in\nFig. 1.1shows multiple building blocks representing the design of the solution to a particular\napplication. This part of the algorithmic design is dependent on the skill of the analyst and\noften uses one or more of the four major problems as a building block. This is, of course,\nnot always the case, but it is frequent enough to merit special treatment of these four\nproblems within this book. To explain the data mining process, we will use an example\nfrom a recommendation scenario.\nExample 1.2.1 Consider a scenario in which a retailer has Web logs corresponding to\ncustomer accesses to Web pages at his or her site. Each of these Web pages corresponds\nto a product, and therefore a customer access to a page may often be indicative of interest\nin that particular product. The retailer also stores demographic proles for the dierent",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\nin that particular product. The retailer also stores demographic proles for the dierent\ncustomers. The retailer wants to make targeted product recommendations to customers using\nthe customer demographics and buying behavior.\nSample Solution Pipeline In this case, the rst step for the analyst is to collect the\nrelevant data from two dierent sources. The rst source is the set of Web logs at the\nsite. The second is the demographic information within the retailer database that were\ncollected during Web registration of the customer. Unfortunately, these data sets are in\na very dierent format and cannot easily be used together for processing. For example,\nconsider a sample log entry of the following form:\n98.206.207.157 - - [31/Jul/2013:18:09:38 -0700] \"GET /productA.htm\nHTTP/1.1\" 200 328177 \"-\" \"Mozilla/5.0 (Mac OS X) AppleWebKit/536.26\n(KHTML, like Gecko) Version/6.0 Mobile/10B329 Safari/8536.25\"\n\"retailer.net\"",
           "Ch 1",
           5
          ],
          [
           "Chapter 1: An Introduction to Data Mining\n(KHTML, like Gecko) Version/6.0 Mobile/10B329 Safari/8536.25\"\n\"retailer.net\"\nThe log may contain hundreds of thousands of such entries. Here, a customer at IP address\n98.206.207.157 has accessed productA.htm. The customer from the IP address can be iden-\ntied using the previous login information, by using cookies, or by the IP address itself,\nbut this may be a noisy process and may not always yield accurate results. The analyst\nwould need to design algorithms for deciding how to lter the dierent log entries and use\nonly those which provide accurate results as a part of thecleaning and extractionprocess.\nFurthermore, the raw log contains a lot of additional information that is not necessarily\n1.2. THE DATA MINING PROCESS 5\nof any use to the retailer. In thefeature extractionprocess, the retailer decides to create\none record for each customer, with a specic choice of features extracted from the Web",
           "Ch 1",
           5
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ch 1",
         "marker": {
          "color": "#636EFA",
          "size": {
           "bdata": "BQUFBQUFBQUFBQUFBQUF",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 1",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "rBdxnNRqxD9SQQWrzhiIP9E2LpV7org/SGJpcQydzD+r6OQxE+fIP4NlbRZJpbU/ncDDXDlH0T9cSgv4kmnGP13oYdziLtE/hciTNO7v0T9dzxgBbzLQPwtmiw7CNsU/OTPN4RuKwT/ilSdkKHzKP7JGcGyPKsY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "830kl6Fq3b9ip+PKC+bVv7hQ5/rAWte/K7VCTREz4b9f5T6uTGTdvwf6BQzzadK/raSc3QMf1L80mtJTgpHevwPfkjwvNeC/Qd7131IW37/w7yAbnCrXvyYplUJEPdu/LSfDjc3Y1b9Xb2gHQU/Sv2ooiG8U6NG/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "APLlAAaqsj9VDNHdAHitP5qRfvvScqQ/iFc7z/ESij+qcona7Eq7P5ryBwDYj8A/QfGhvFzSzD/urF3/Z/mtPwHNujQZJqG/MtbmVYspwr+vgVqQWsy1v/LZFr002Ki/M5hh3IQmnz/6EMRKKGufP/OIbDFtQ8W/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Chapter 2: Data Preparation\nChapter 2\nData Preparation\nSuccess depends upon previous preparation, and without such\npreparation there is sure to be failure.Confucius\n2.1 Introduction\nThe raw format of real data is usually widely variable. Many values may be missing, incon-\nsistent across dierent data sources, and erroneous. For the analyst, this leads to numerous\nchallenges in using the data eectively. For example, consider the case of evaluating the\ninterests of consumers from their activity on a social media site. The analyst may rst\nneed to determine the types of activity that are valuable to the mining process. The activ-\nity might correspond to the interests entered by the user, the comments entered by the\nuser, and the set of friendships of the user along with their interests. All these pieces of\ninformation are diverse and need to be collected from dierent databases within the social\nmedia site. Furthermore, some forms of data, such as raw logs, are often not directly usable",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nmedia site. Furthermore, some forms of data, such as raw logs, are often not directly usable\nbecause of their unstructured nature. In other words, useful features need to be extracted\nfrom these data sources. Therefore, adata preparation phaseis needed.\nThe data preparation phase is a multistage process that comprises several individual\nsteps, some or all of which may be used in a given application. These steps are as follows:\n1. Feature extraction and portability:The raw data is often in a form that is not suit-\nable for processing. Examples include raw logs, documents, semistructured data, and\npossibly other forms of heterogeneous data. In such cases, it may be desirable to\nderive meaningful features from the data. Generally, features with good semantic\ninterpretability are more desirable because they simplify the ability of the analyst\nto understand intermediate results. Furthermore, they are usually better tied to the",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nto understand intermediate results. Furthermore, they are usually better tied to the\ngoals of the data mining application at hand. In some cases where the data is obtained\nfrom multiple sources, it needs to be integrated into a single database for processing.\nIn addition, some algorithms may work only with a specic data type, whereas the\ndata may contain heterogeneous types. In such cases,data type portabilitybecomes\nC. C. Aggarwal,Data Mining: The Textbook, DOI 10.1007/978-3-319-14142-822 7\nc Springer International Publishing Switzerland 2015\n28 CHAPTER 2. DATA PREPARATION\nimportant where attributes of one type are transformed to another. This results in a\nmore homogeneous data set that can be processed by existing algorithms.\n2. Data cleaning:In the data cleaning phase, missing, erroneous, and inconsistent entries\nare removed from the data. In addition, some missing entries may also be estimated\nby a process known asimputation.",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nby a process known asimputation.\n3. Data reduction, selection, and transformation:In this phase, the size of the data is\nreduced through data subset selection, feature subset selection, or data transforma-\ntion. The gains obtained in this phase are twofold. First, when the size of the data is\nreduced, the algorithms are generally more ecient. Second, if irrelevant features or\nirrelevant records are removed, the quality of the data mining process is improved. The\nrst goal is achieved by generic sampling and dimensionality reduction techniques. To\nachieve the second goal, a highly problem-specic approach must be used for feature\nselection. For example, a feature selection approach that works well for clustering may\nnot work well for classication.\nSome forms of feature selection are tightly integrated with the problem at hand. Later\nchapters on specic problems such as clustering and classication will contain detailed\ndiscussions on feature selection.",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\ndiscussions on feature selection.\nThischapterisorganizedasfollows.ThefeatureextractionphaseisdiscussedinSect. 2.2.\nThe data cleaning phase is covered in Sect.2.3. The data reduction phase is explained in\nSect. 2.4. A summary is given in Sect.2.5.\n2.2 Feature Extraction and Portability\nThe rst phase of the data mining process is creating a set of features that the analyst can\nwork with. In cases where the data is in raw and unstructured form (e.g., raw text, sensor\nsignals), the relevant features need to be extracted for processing. In other cases where a\nheterogeneous mixture of features is available in dierent forms, an o-the-shelf analytical\napproach is often not available to process such data. In such cases, it may be desirable to\ntransform the data into a uniform representation for processing. This is referred to asdata\ntype porting.\n2.2.1 Feature Extraction\nThe rst phase of feature extraction is a crucial one, though it is very application specic.",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nThe rst phase of feature extraction is a crucial one, though it is very application specic.\nIn some cases, feature extraction is closely related to the concept of data type portability,\nwhere low-level features of one type may be transformed to higher-level features of another\ntype. The nature of feature extraction depends on the domain from which the data is drawn:\n1. Sensor data:Sensor data is often collected as large volumes of low-level signals, which\nare massive. The low-level signals are sometimes converted to higher-level features\nusing wavelet or Fourier transforms. In other cases, the time series is used directly\nafter some cleaning. The eld of signal processing has an extensive literature devoted\nto such methods. These technologies are also useful for porting time-series data to\nmultidimensional data.\n2. Image data: In its most primitive form, image data are represented as pixels. At a\nslightly higher level, color histograms can be used to represent the features in dier-",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nslightly higher level, color histograms can be used to represent the features in dier-\nent segments of an image. More recently, the use ofvisual words has become more\n2.2. FEATURE EXTRACTION AND PORTABILITY 29\npopular. This is a semantically rich representation that is similar to document data.\nOne challenge in image processing is that the data are generally very high dimen-\nsional. Thus, feature extraction can be performed at dierent levels, depending on the\napplication at hand.\n3. Web logs:Web logs are typically represented as text strings in a prespecied format.\nBecause the elds in these logs are clearly specied and separated, it is relatively easy\nto convert Web access logs into a multidimensional representation of (the relevant)\ncategorical and numeric attributes.\n4. Network trac:In many intrusion-detection applications, the characteristics of the\nnetwork packets are used to analyze intrusions or other interesting activity. Depending",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nnetwork packets are used to analyze intrusions or other interesting activity. Depending\non the underlying application, a variety of features may be extracted from these\npackets, such as the number of bytes transferred, the network protocol used, and so\non.\n5. Document data:Document data is often available in raw and unstructured form, and\nthe data may contain rich linguistic relations between dierent entities. One approach\nis to remove stop words, stem the data, and use a bag-of-words representation. Other\nmethods useentity extractionto determine linguistic relationships.\nNamed-entity recognition is an important subtask of information extraction. This\napproach locates and classies atomic elements in text into predened expressions\nof names of persons, organizations, locations, actions, numeric quantities, and so on.\nClearly, the ability to identify such atomic elements is very useful because they can be",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nClearly, the ability to identify such atomic elements is very useful because they can be\nused to understand the structure of sentences and complex events. Such an approach\ncan also be used to populate a more conventional database of relational elements or\nas a sequence of atomic entities, which is more easily analyzed. For example, consider\nthe following sentence:\nBill Clinton lives in Chappaqua.\nHere, Bill Clinton is the name of a person, and Chappaqua is the name of a\nplace. The word lives denotes an action. Each type of entity may have a dierent\nsignicance to the data mining process depending on the application at hand. For\nexample, if a data mining application is mainly concerned with mentions of specic\nlocations, then the word Chappaqua needs to be extracted.\nPopular techniques for named entity recognition include linguistic grammar-based\ntechniques and statistical models. The use of grammar rules is typically very eective,",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\ntechniques and statistical models. The use of grammar rules is typically very eective,\nbut it requires work by experienced computational linguists. On the other hand, sta-\ntistical models require a signicant amount of training data. The techniques designed\nare very often domain-specic. The area of named entity recognition is vast in its own\nright, which is outside the scope of this book. The reader is referred to [400]f o ra\ndetailed discussion of dierent methods for entity recognition.\nFeature extraction is an art form that is highly dependent on the skill of the analyst to\nchoose the features and their representation that are best suited to the task at hand. While\nthis particular aspect of data analysis typically belongs to the domain expert, it is perhaps\nthe most important one. If the correct features are not extracted, the analysis can only be\nas good as the available data.\n30 CHAPTER 2. DATA PREPARATION\n2.2.2 Data Type Portability",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nas good as the available data.\n30 CHAPTER 2. DATA PREPARATION\n2.2.2 Data Type Portability\nData type portability is a crucial element of the data mining process because the data is\noften heterogeneous, and may contain multiple types. For example, a demographic data\nset may contain both numeric and mixed attributes. A time-series data set collected from\nan electrocardiogram (ECG) sensor may have numerous other meta-information and text\nattributes associated with it. This creates a bewildering situation for an analyst who is now\nfaced with the dicult challenge of designing an algorithm with an arbitrary combination\nof data types. The mixing of data types also restricts the ability of the analyst to use\no-the-shelf tools for processing. Note that porting data types does lose representational\naccuracy and expressiveness in some cases. Ideally, it is best to customize the algorithm\nto the particular combination of data types to optimize results. This is, however, time-",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nto the particular combination of data types to optimize results. This is, however, time-\nconsuming and sometimes impractical.\nThis section will describe methods for converting between various data types. Because\nthe numeric data type is the simplest and most widely studied one for data mining algo-\nrithms, it is particularly useful to focus on how dierent data types may be converted to\nit. However, other forms of conversion are also useful in many scenarios. For example, for\nsimilarity-based algorithms, it is possible to convert virtually any data type to a graph and\napply graph-based algorithms to this representation. The following discussion, summarized\nin Table2.1, will discuss various ways of transforming data across dierent types.\n2.2.2.1 Numeric to Categorical Data: Discretization\nThe most commonly used conversion is from the numeric to the categorical data type.\nThis process is known asdiscretization. The process of discretization divides the ranges of",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nThis process is known asdiscretization. The process of discretization divides the ranges of\nthe numeric attribute intoranges. Then, the attribute is assumed to containdierent\ncategorical labeled values from 1 to, depending on the range in which the original attribute\nlies. For example, consider the age attribute. One could create ranges [0,10], [11,20], [21,30],\nand so on. Thesymbolic value for any record in the range [11,20] is 2 and the symbolic\nvalue for a record in the range [21,30] is 3. Because these are symbolic values, no ordering\nis assumed between the values 2 and 3. Furthermore, variations within a range are\nnot distinguishable after discretization. Thus, the discretization process does lose some\ninformationfortheminingprocess.However,forsomeapplications,thislossofinformationis\nnot too debilitating. One challenge with discretization is that the data may be nonuniformly\ndistributed across the dierent intervals. For example, for the case of the salary attribute,",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\ndistributed across the dierent intervals. For example, for the case of the salary attribute,\na large subset of the population may be grouped in the [40,000, 80,000] range, but very\nfew will be grouped in the [1,040,000, 1,080,000] range. Note that both ranges have the\nsame size. Thus, the use of ranges of equal size may not be very helpful in discriminating\nbetween dierent data segments. On the other hand, many attributes, such as age, are not\nas nonuniformly distributed, and therefore ranges of equal size may work reasonably well.\nThe discretization process can be performed in a variety of ways depending on application-\nspecic goals:\n1. Equi-width ranges: In this case, each range [a,b] is chosen in such a way thatb a\nis the same for each range. This approach has the drawback that it will not work for\ndata sets that are distributed nonuniformly across the dierent ranges. To determine\nthe actual values of the ranges, the minimum and maximum values of each attribute",
           "Ch 2",
           5
          ],
          [
           "Chapter 2: Data Preparation\nthe actual values of the ranges, the minimum and maximum values of each attribute\nare determined. This range [min,max] is then divided intoranges of equal length.\n2. Equi-log ranges:Each range [a,b] is chosen in such a way that log(b)log(a)h a st h e\nsame value. This kinds of range selection has the eect of geometrically increasing\n2.2. FEATURE EXTRACTION AND PORTABILITY 31\nTable 2.1: Portability of dierent data types\nSource data type Destination data type Methods\nNumeric Categorical Discretization\nCategorical Numeric Binarization\nText Numeric Latent semantic analysis (LSA)\nTime series Discrete sequence SAX\nTime series Numeric multidimensional DWT, DFT\nDiscrete sequence Numeric multidimensional DWT, DFT\nSpatial Numeric multidimensional 2-d DWT\nGraphs Numeric multidimensional MDS, spectral\nAny type Graphs Similarity graph\n(Restricted applicability)\nranges [a,a ], [a,a2], and so on, for some> 1. This kind of range may be",
           "Ch 2",
           5
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ch 2",
         "marker": {
          "color": "#EF553B",
          "size": {
           "bdata": "BQUFBQUFBQUFBQUFBQUF",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 2",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "alXbtLfV0j/SmQQ0y5XZP+gvD1qyCNM/4YF2RTeQzj8x/wDHEnvYP1MMRSRkOdU/RgZC52cD0T+VI482rufMP7xeIDuN+8E/EzbtxzKv0T+QRAdS6QDTP8ATSTEatss/qrUZK/3wxj+BEEguihPLP1A6jdcR/9I/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ZEeTq/Xx2L/1KQWAcy7Wv10dbCpPNtq/lLptKbqrw7+jUF4MfjLZv7hlmKMXgNK/O9IID0cszL+PhuXwRdDRv2q8F0crStC/pBkJTpxMw79ITUc6wD/Xv+lTgFIwoc6/hctKSZsjrr8f49MiYFG5P4JfvDVC1LW/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "8CVceVo7rb81hmOwmBPQvyYMLipW+Fa/KcaS7buvzb+A/leRfZrOv9JrLxB/E8G/XGOzRhupsr9zXNTcidPCv5kpOOLP37C/MHbADD1J0L86mF32SoiyPzxmK/5HWLE/n7INXVuKlT/SBzvGkOGwP33GJYFoup2/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Chapter 3: Similarity and Distances\nChapter 3\nSimilarity and Distances\nLove is the power to see similarity in the dissimilar.Theodor Adorno\n3.1 Introduction\nMany data mining applications require the determination of similar or dissimilar objects,\npatterns, attributes, and events in the data. In other words, a methodical way of quanti-\nfying similarity between data objects is required. Virtually all data mining problems, such\nas clustering, outlier detection, and classication, require the computation of similarity. A\nformal statement of the problem of similarity or distance quantication is as follows:\nGiven two objects O\n1 and O2, determine a value of the similarity Sim(O1,O2) (or dis-\ntance Dist(O1,O2)) between the two objects.\nIn similarity functions, larger values imply greater similarity, whereas in distance func-\ntions, smaller values imply greater similarity. In some domains, such as spatial data, it is\nmore natural to talk about distance functions, whereas in other domains, such as text, it is",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nmore natural to talk about distance functions, whereas in other domains, such as text, it is\nmore natural to talk about similarity functions. Nevertheless, the principles involved in the\ndesign of such functions are generally invariant across dierent data domains. This chap-\nter will, therefore, use either of the terms distance function and similarity function,\ndepending on the domain at hand. Similarity and distance functions are often expressed in\nclosed form (e.g., Euclidean distance), but in some domains, such as time-series data, they\nare dened algorithmically and cannot be expressed in closed form.\nDistance functions are fundamental to the eective design of data mining algorithms,\nbecause a poor choice in this respect may be very detrimental to the quality of the results.\nSometimes, data analysts use the Euclidean function as a black box without much thought\nabout the overall impact of such a choice. It is not uncommon for an inexperienced analyst",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nabout the overall impact of such a choice. It is not uncommon for an inexperienced analyst\nto invest signicant eort in the algorithmic design of a data mining problem, while treating\nthe distance function subroutine as an afterthought. This is a mistake. As this chapter will\nelucidate, poor choices of the distance function can sometimes be disastrously misleading\nC. C. Aggarwal,Data Mining: The Textbook, DOI 10.1007/978-3-319-14142-836 3\nc Springer International Publishing Switzerland 2015\n64 CHAPTER 3. SIMILARITY AND DISTANCES\ndepending on the application domain. Good distance function design is also crucial for type\nportability. As discussed in Sect.2.4.4.3 of Chap. 2, spectral embedding can be used to\nconvert a similarity graph constructed on any data type into multidimensional data.\nDistance functions are highly sensitive to the data distribution, dimensionality, and data\ntype. In some data types, such as multidimensional data, it is much simpler to dene and",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\ntype. In some data types, such as multidimensional data, it is much simpler to dene and\ncompute distance functions than in other types such as time-series data. In some cases,\nuser intentions (ortraining feedbackon object pairs) are available tosupervise the distance\nfunction design. Although this chapter will primarily focus on unsupervised methods, we\nwill also briey touch on the broader principles of using supervised methods.\nThis chapter is organized as follows. Section3.2 studies distance functions for multidi-\nmensional data. This includes quantitative, categorical, and mixed attribute data. Similarity\nmeasures for text, binary, and set data are discussed in Sect.3.3. Temporal data is discussed\nin Sect. 3.4. Distance functions for graph data are addressed in Sect.3.5. A discussion of\nsupervised similarity will be provided in Sect.3.6. Section3.7 gives a summary.\n3.2 Multidimensional Data",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\n3.2 Multidimensional Data\nAlthough multidimensional data are the simplest form of data, there is signicant diversity\nindistancefunctiondesignacrossdierentattributetypessuchascategoricalorquantitative\ndata. This section will therefore study each of these types separately.\n3.2.1 Quantitative Data\nThe most common distance function for quantitative data is theLp-norm. The Lp-norm\nbetween two data pointsX =( x1 ...x d)a n dY =( y1 ...y d)i sd e  n e da sf o l l o w s :\nDist(X,Y)=\n( d\ni=1\n|xi yi|p\n)1/p\n. (3.1)\nTwo special cases of theLp-norm are theEuclidean (p =2 )a n dt h eManhattan (p =1 )\nmetrics. These special cases derive their intuition from spatial applications where they have\nclear physical interpretability. The Euclidean distance is the straight-line distance between\ntwo data points. The Manhattan distance is the city block driving distance in a region in\nwhich the streets are arranged as a rectangular grid, such as the Manhattan Island of New\nYork City.",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nYork City.\nA nice property of the Euclidean distance is that it is rotation-invariant because the\nstraight-line distance between two data points does not change with the orientation of the\naxis system. This property also means that transformations, such asPCA, SVD,o rt h e\nwavelet transformation for time series (discussed in Chap. 2), can be used on the data\nwithout aecting\n1 the distance. Another interesting special case is that obtained by setting\np = . The result of this computation is to select the dimension for which the two objects\nare the most distant from one another and report the absolute value of this distance. All\nother features are ignored.a\nTheL\np-norm is one of the most popular distance functions used by data mining analysts.\nOne of the reasons for its popularity is the natural intuitive appeal and interpretability of\nL1-a n dL2-norms in spatial applications. The intuitive interpretability of these distances",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nL1-a n dL2-norms in spatial applications. The intuitive interpretability of these distances\ndoes not, however, mean that they are the most relevant ones, especially for the high-\ndimensional case. In fact, these distance functions may not work very well when the data\n1The distances are aected after dimensions are dropped. However, the transformation itself does not\nimpact distances.\n3.2. MULTIDIMENSIONAL DATA 65\nFigure 3.1: Reduction in distance contrasts with increasing dimensionality and norms\nare high dimensional because of the varying impact of data sparsity, distribution, noise,\nand feature relevance. This chapter will discuss these broader principles in the context of\ndistance function design.\n3.2.1.1 Impact of Domain-Specic Relevance\nIn some cases, an analyst may know which features are more important than others for a\nparticular application. For example, for a credit-scoring application, an attribute such as",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nparticular application. For example, for a credit-scoring application, an attribute such as\nsalary is much more relevant to the design of the distance function than an attribute such\nas gender, though both may have some impact. In such cases, the analyst may choose to\nweight the features dierently if domain-specic knowledge about the relative importance\nof dierent features is available. This is often a heuristic process based on experience and\nskill. The generalizedL\np-distance is most suitable for this case and is dened in a similar\nway to theLp-norm, except that a coecientai is associated with theith feature. This\ncoecient is used to weight the corresponding feature component in theLp-norm:\nDist(X,Y)=\n( d\ni=1\nai |xi yi|p\n)1/p\n. (3.2)\nThis distance is also referred to as the generalizedMinkowski distance. In many cases, such\ndomain knowledge is not available. Therefore, theLp-norm may be used as a default option.",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\ndomain knowledge is not available. Therefore, theLp-norm may be used as a default option.\nUnfortunately, without knowledge about the most relevant features, theLp-norm is suscep-\ntible to some undesirable eects of increasing dimensionality, as discussed subsequently.\n3.2.1.2 Impact of High Dimensionality\nMany distance-based data mining applications lose their eectiveness as the dimensionality\nof the data increases. For example, a distance-based clustering algorithm may group unre-\nlated data points because the distance function may poorly reect the intrinsic semantic\ndistances between data points with increasing dimensionality. As a result, distance-based\nmodels of clustering, classication, and outlier detection are oftenqualitatively ineective.\nThis phenomenon is referred to as the curse of dimensionality, a term rst coined by\nRichard Bellman.\n66 CHAPTER 3. SIMILARITY AND DISTANCES\nTo better understand the impact of the dimensionality curse on distances, let us examine",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nTo better understand the impact of the dimensionality curse on distances, let us examine\na unit cube of dimensionalityd that is fully located in the nonnegative quadrant, with one\ncorner at the originO. What is the Manhattan distance of the corner of this cube (say, at\nthe origin) to a randomly chosen pointX inside the cube? In this case, because one end\npoint is the origin, and all coordinates are nonnegative, the Manhattan distance will sum up\nthe coordinates ofX over the dierent dimensions. Each of these coordinates is uniformly\ndistributed in [0,1]. Therefore, ifYi represents the uniformly distributed random variable\nin [0,1], it follows that the Manhattan distance is as follows:\nDist(O,X)=\nd\ni=1\n(Yi 0). (3.3)\nThe result is a random variable with a mean of = d/2 and a standard deviation of\n=\n\nd/12. For large values ofd, it can be shown by the law of large numbers that the vast\nmajority of randomly chosen points inside the cube will lie in the range [Dmin,Dmax]=",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nmajority of randomly chosen points inside the cube will lie in the range [Dmin,Dmax]=\n[3,+3 ]. Therefore, most of the points in the cube lie within a distance range of\nDmax Dmin =6 =\n\n3d from the origin. Note that the expected Manhattan distance\ngrows with dimensionality at a rate that is linearly proportional tod. Therefore, theratio\nof the variation in the distances to the absolute values that is referred to asContrast(d),\nis given by:\nContrast(d)= Dmax Dmin\n =\n\n12/d. (3.4)\nThis ratio can be interpreted as the distancecontrast between the dierent data points,\nin terms of how dierent the minimum and maximum distances from the origin might\nbe considered. Because the contrast reduces with\n\nd, it means that there is virtually no\ncontrast with increasing dimensionality. Lower contrasts are obviously not desirable because\nit means that the data mining algorithm will score the distances between all pairs of data",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nit means that the data mining algorithm will score the distances between all pairs of data\npoints in approximately the same way and will not discriminate well between dierent\npairs of objects with varying levels of semantic relationships. The variation in contrast with\nincreasing dimensionality is shown in Fig.3.1a. This behavior is, in fact, observed for all\nL\np-norms at dierent values ofp, though with varying severity. These dierences in severity\nwill be explored in a later section. Clearly, with increasing dimensionality, a direct use of\nthe L\np-norm may not be eective.\n3.2.1.3 Impact of Locally Irrelevant Features\nA more fundamental way of exploring the eects of high dimensionality is by examining the\nimpact of irrelevant features. This is because many features are likely to be irrelevant in a\ntypical high-dimensional data set. Consider, for example, a set of medical records, contain-\ning patients with diverse medical conditions and very extensive quantitative measurements",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\ning patients with diverse medical conditions and very extensive quantitative measurements\nabout various aspects of an individuals medical history. For a cluster containing diabetic\npatients, certain attributes such as the blood glucose level are more important for the dis-\ntance computation. On the other hand, for a cluster containing epileptic patients, a dierent\nset of features will be more important. The additive eects of the natural variations in the\nmany attribute values may be quite signicant. A distance metric such as the Euclidean\nmetric may unnecessarily contribute a high value from the more noisy components because\nof its square-sum approach. The key point to understand here is that the precise features\nthat are relevant to the distance computation may sometimes be sensitive to the particular\npair of objects that are being compared. This problem cannot be solved by global feature\n3.2. MULTIDIMENSIONAL DATA 67\nFigure 3.2: Impact ofp on contrast",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\n3.2. MULTIDIMENSIONAL DATA 67\nFigure 3.2: Impact ofp on contrast\nsubset selection during preprocessing, because the relevance of features islocallydetermined\nby the pair of objects that are being considered. Globally, all features may be relevant.\nWhenmanyfeaturesareirrelevant,theadditivenoiseeectsoftheirrelevantfeaturescan\nsometimes be reected in the concentration of the distances. In any case, such irrelevant fea-\ntures will almost always result in errors in distance computation. Because high-dimensional\ndata sets are often likely to contain diverse features, many of which are irrelevant, the addi-\ntive eect with the use of a sum-of-squares approach, such as theL\n2-norm, can be very\ndetrimental.\n3.2.1.4 Impact of Dierent Lp-Norms\nDierentLp-norms do not behave in a similar way either in terms of the impact of irrelevant\nfeatures or the distance contrast. Consider the extreme case whenp = . This translates to",
           "Ch 3",
           5
          ],
          [
           "Chapter 3: Similarity and Distances\nfeatures or the distance contrast. Consider the extreme case whenp = . This translates to\nusing only the dimension where the two objects are the mostdissimilar. Very often, this may\nbe the impact of the natural variations in an irrelevant attribute that is not too useful for a\nsimilarity-based application. In fact, for a 1000-dimensional application, if two objects have\nsimilar values on 999 attributes, such objects should be consideredvery similar. However,\na single irrelevant attribute on which the two objects are very dierent will throw o the\ndistance value in the case of theL\n metric. In other words, local similarity properties of\nthe data are de-emphasized byL. Clearly, this is not desirable.\nThis behavior is generally true for larger values ofp, where the irrelevant attributes\nare emphasized. In fact, it can also be shown that distance contrasts are also poorer for\nlarger values ofp for certain data distributions. In Fig.3.1b, the distance contrasts have",
           "Ch 3",
           5
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ch 3",
         "marker": {
          "color": "#00CC96",
          "size": {
           "bdata": "BQUFBQUFBQUFBQUFBQUF",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 3",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "Pn+GgbYZzj+4+ORosMLQPy1u4AMwQNQ/xzMNwGOo0j/Nh2hVMBfXP1oQT7y4KdM/3d+b8rzO1z8CGoQCJJXUP01jfKie1dM/PtctKUAA0D8WPrKx/ZfNP5NHZv7oGtQ/zSK65f2e1j/7/dy+Q9HWP2nOQ36rRNI/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "2TEbscrfyT/YEGwGNaLKP0HupWFMW8M/HVQ3TgO9xz+OPNU/yhvOP0K1D6d8l9A/1yZ3LLQN1D8x5gQATafZP2Nr54wPnNM/AYX7QFT+0j+9C/mKCrDNP45wBqQZK88/pNcgG8Ly2j+5dNzvexfVP2TbOAdxOdo/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "Q9VlG4Nx0z9hnKSnwrzUPwbUNXca4dM/TlbTHA6d0z+xZ+v3suPbP5MqVMAtXtg/EViBLTbo1T8LQhwfhgrJP8nHY/dfVdI/wHG69WwB2D/wHh03S0fZPzloqBR6+dM/RV9ot5UUwD+596YT7Z/QP5wH/EoNX9c/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Chapter 4: Association Pattern Mining\n94 CHAPTER 4. ASSOCIATION PATTERN MINING\n4. Other major data mining problems:Frequent pattern mining can be used as a subrou-\ntine to provide eective solutions to many data mining problems such as clustering,\nclassication, and outlier analysis.\nBecause the frequent pattern mining problem was originally proposed in the context of\nmarket basket data, a signicant amount of terminology used to describe both the data (e.g.,\ntransactions) and the output (e.g.,itemsets) is borrowed from the supermarket analogy.\nFrom an application-neutral perspective, a frequent pattern may be dened as a frequent\nsubset, dened on the universe of all possible sets. Nevertheless, because the market basket\nterminology has been used popularly, this chapter will be consistent with it.\nFrequent itemsets can be used to generateassociation rulesof the formX  Y,w h e r e\nX and Y are sets of items. A famous example of an association rule, which has now become\npart",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nX and Y are sets of items. A famous example of an association rule, which has now become\npart\n1 of the data mining folklore, is{Beer}{ Diapers}. This rule suggests that buying\nbeer makes it more likely that diapers will also be bought. Thus, there is a certain direc-\ntionality to the implication that is quantied as a conditional probability. Association rules\nare particularly useful for a variety of target market applications. For example, if a super-\nmarket owner discovers that{Eggs,Milk }{ Yogurt } is an association rule, he or she can\npromote yogurt to customers who often buy eggs and milk. Alternatively, the supermarket\nowner may place yogurt on shelves that are located in proximity to eggs and milk.\nThe frequency-based model for association pattern mining is very popular because of its\nsimplicity. However, the raw frequency of a pattern is not quite the same as the statistical\nsignicance of the underlying correlations. Therefore, numerous models for frequent pattern",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nsignicance of the underlying correlations. Therefore, numerous models for frequent pattern\nmining have been proposed that are based on statistical signicance. This chapter will also\nexplore some of these alternative models, which are also referred to asinteresting patterns.\nThis chapter is organized as follows. Section4.2 introduces the basic model for associa-\ntion pattern mining. The generation of association rules from frequent itemsets is discussed\nin Sect.4.3. A variety of algorithms for frequent pattern mining are discussed in Sect.4.4.\nThis includes theApriori algorithm, a number of enumeration tree algorithms, and a sux-\nbased recursive approach. Methods for nding interesting frequent patterns are discussed in\nSect. 4.5. Meta-algorithms for frequent pattern mining are discussed in Sect.4.6. Section4.7\ndiscusses the conclusions and summary.\n4.2 The Frequent Pattern Mining Model\nThe problem of association pattern mining is naturally dened on unordered set-wise data.",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nThe problem of association pattern mining is naturally dened on unordered set-wise data.\nIt is assumed that the databaseT contains a set ofn transactions, denoted byT1 ...T n.\nEach transaction Ti is drawn on the universe of itemsU and can also be represented as\na multidimensional record of dimensionality,d = |U|, containing only binary attributes.\nEach binary attribute in this record represents a particular item. The value of an attribute\nin this record is 1 if that item is present in the transaction, and 0 otherwise. In practical\nsettings, the universe of itemsU is very large compared to the typical number of items in\neach transactionT\ni. For example, a supermarket database may have tens of thousands of\nitems, and a single transaction will typically contain less than 50 items. This property is\noften leveraged in the design of frequent pattern mining algorithms.\nAn itemset is a set of items. Ak-itemset is an itemset that contains exactlyk items.",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nAn itemset is a set of items. Ak-itemset is an itemset that contains exactlyk items.\nIn other words, ak-itemset is a set of items of cardinalityk. The fraction of transactions\n1This rule was derived in some early publications on supermarket data. No assertion is made here about\nthe likelihood of such a rule appearing in an arbitrary supermarket data set.\n4.2. THE FREQUENT PATTERN MINING MODEL 95\nTable 4.1: Example of a snapshot of a market basket data set\ntid Set of items Binary representation\n1 {Bread,Butter,Milk } 110010\n2 {Eggs,Milk,Yogurt } 000111\n3 {Bread,Cheese,Eggs,Milk } 101110\n4 {Eggs,Milk,Yogurt } 000111\n5 {Cheese,Milk,Yogurt} 001011\nin T1 ...T n in which an itemset occurs as a subset provides a crisp quantication of its\nfrequency. This frequency is also known as thesupport.\nDenition 4.2.1 (Support) The support of an itemsetI is dened as the fraction of the\ntransactions in the databaseT = {T1 ...T n} that containI as a subset.",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\ntransactions in the databaseT = {T1 ...T n} that containI as a subset.\nThe support of an itemsetI is denoted bysup(I). Clearly, items that are correlated will\nfrequently occur together in transactions. Such itemsets will have high support. Therefore,\nthe frequent pattern mining problem is that of determining itemsets that have the requisite\nlevel ofminimum support.\nDenition 4.2.2 (Frequent Itemset Mining)Given a set of transactions T =\n{T1 ...T n}, where each transaction Ti is a subset of items from U, determine all item-\nsets I that occur as a subset of at least a predened fractionminsup of the transactions in\nT .\nThe predened fractionminsup is referred to as the minimum support. While the default\nconvention in this book is to assume thatminsup refers to a fractional relative value, it\nis also sometimes specied as an absolute integer value in terms of the raw number of\ntransactions. This chapter will always assume the convention of a relative value, unless",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\ntransactions. This chapter will always assume the convention of a relative value, unless\nspecied otherwise. Frequent patterns are also referred to as frequent itemsets, or large\nitemsets. This book will use these terms interchangeably.\nThe unique identier of a transaction is referred to as atransaction identier,o rtid for\nshort. The frequent itemset mining problem may also be stated more generally in set-wise\nform.\nDenition 4.2.3 (Frequent Itemset Mining: Set-wise Denition)Given a set of\nsets T = {T\n1 ...T n}, where each element of the setTi is drawn on the universe of ele-\nments U, determine all setsI t h a to c c u ra sas u b s e to fa tl e a s tap r e d e  n e df r a c t i o nminsup\nof the sets inT .\nAs discussed in Chap. 1, binary multidimensional data and set data are equivalent. This\nequivalence is because each multidimensional attribute can represent a set element (or\nitem). A value of 1 for a multidimensional attribute corresponds to inclusion in the set (or",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nitem). A value of 1 for a multidimensional attribute corresponds to inclusion in the set (or\ntransaction). Therefore, a transaction data set (or set of sets) can also be represented as a\nmultidimensional binary database whose dimensionality is equal to the number of items.\nConsider the transactions illustrated in Table4.1. Each transaction is associated with a\nunique transaction identier in the leftmost column, and contains a baskets of items that\nwere bought together at the same time. The right column in Table4.1 contains the binary\nmultidimensional representation of the corresponding basket. The attributes of this binary\nrepresentation are arranged in the order{Bread, Butter,Cheese, Eggs,Milk, Yogurt}.I n\n96 CHAPTER 4. ASSOCIATION PATTERN MINING\nthis database of 5 transactions, thesupport of {Bread,Milk } is 2/5=0 .4b e c a u s eb o t h\nitems in this basket occur in 2 out of a total of 5 transactions. Similarly, the support of",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nitems in this basket occur in 2 out of a total of 5 transactions. Similarly, the support of\n{Cheese,Yogurt} is 0.2 because it appears in only the last transaction. Therefore, if the\nminimum support is set to 0.3, then the itemset{Bread,Milk } will be reported but not\nthe itemset{Cheese,Yogurt}.\nThe number of frequent itemsets is generally very sensitive to the minimum support\nlevel. Consider the case where a minimum support level of 0.3 is used. Each of the items\nBread, Milk, Eggs, Cheese,a n dYogurt occur in more than 2 transactions, and can\ntherefore be considered frequent items at a minimum support level of 0.3. These items\nare frequent 1-itemsets. In fact, the only item that is not frequent at a support level of\n0.3i sButter . Furthermore, the frequent 2-itemsets at a minimum support level of 0.3a r e\n{Bread,Milk },{Eggs,Milk },{Cheese,Milk },{Eggs,Yogurt },a n d{Milk,Yogurt }.T h e\nonly3-itemsetreportedatasupportlevelof0 .3is{Eggs,Milk,Yogurt }.Ontheotherhand,",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nonly3-itemsetreportedatasupportlevelof0 .3is{Eggs,Milk,Yogurt }.Ontheotherhand,\nif the minimum support level is set to 0.2, it corresponds to an absolute support value of\nonly 1. In such a case, every subset of every transaction will be reported. Therefore, the use\nof lower minimum support levels yields a larger number of frequent patterns. On the other\nhand, if the support level is too high, then no frequent patterns will be found. Therefore, an\nappropriate choice of the support level is crucial for discovering a set of frequent patterns\nwith meaningful size.\nWhen an itemsetI is contained in a transaction, all its subsets will also be contained\nin the transaction. Therefore, the support of any subsetJ of I will always be at least equal\nto that ofI. This property is referred to as thesupport monotonicity property.\nProperty 4.2.1(Supp\nort Monotonicity Property)The support of every subsetJ of\nI is at least equal to that of the support of itemsetI.\nsup(J) sup(I) J I (4.1)",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nI is at least equal to that of the support of itemsetI.\nsup(J) sup(I) J I (4.1)\nThe monotonicity property of support implies that every subset of a frequent itemset will\nalso be frequent. This is referred to as thedownward closure property.\nProperty 4.2.2 (Downward Closure Property)Every subset of a frequent itemset is\nalso frequent.\nThe downward closure property of frequent patterns is algorithmically very convenient\nbecause it provides an important constraint on the inherent structure of frequent patterns.\nThis constraint is often leveraged by frequent pattern mining algorithms to prune the search\nprocess and achieve greater eciency. Furthermore, the downward closure property can\nbe used to create concise representations of frequent patterns, wherein only themaximal\nfrequent subsets are retained.\nDenition 4.2.4 (Maximal Frequent Itemsets)A frequent itemset is maximal at a\ngiven minimum support levelminsup, if it is frequent, and no superset of it is frequent.",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\ngiven minimum support levelminsup, if it is frequent, and no superset of it is frequent.\nIn the example of Table4.1, the itemset{Eggs,Milk,Yogurt } is a maximal frequent item-\nset at a minimum support level of 0.3. However, the itemset{Eggs,Milk } is not maxi-\nmal because it has a superset that is also frequent. Furthermore, the set ofmaximal fre-\nquent patterns at a minimum support level of 0.3 is{Bread,Milk }, {Cheese,Milk },a n d\n{Eggs,Milk,Yogurt }. Thus, there are only 3 maximal frequent itemsets, whereas the num-\nber of frequent itemsets in the entire transaction database is 11. All frequent itemsets can\nbe derived from the maximal patterns by enumerating the subsets of the maximal frequent\n4.3. ASSOCIATION RULE GENERATION FRAMEWORK 97\nFigure 4.1: The itemset lattice\npatterns. Therefore, the maximal patterns can be considered condensed representations of\nthe frequent patterns. However, this condensed representation does not retain information",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nthe frequent patterns. However, this condensed representation does not retain information\nabout the support values of the subsets. For example, the support of{Eggs,Milk,Yogurt }\nis 0.4, but it does not provide any information about the support of{Eggs,Milk },w h i c hi s\n0.6. A dierent condensed representation, referred to asclosed frequent itemsets,i sa b l et o\nretain support information as well. The notion of closed frequent itemsets will be studied\nin detail in Chap.5.\nAn interesting property of itemsets is that they can be conceptually arranged in the form\nof alattice of itemsets. This lattice contains one node for each of the 2\n|U| sets drawn from\nthe universe of itemsU. An edge exists between a pair of nodes, if the corresponding sets\ndier by exactly one item. An example of an itemset lattice of size 25 =3 2o nau n i v e r s eo f\n5 items is illustrated in Fig.4.1. The lattice represents the search space of frequent patterns.",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\n5 items is illustrated in Fig.4.1. The lattice represents the search space of frequent patterns.\nAll frequent pattern mining algorithms, implicitly or explicitly, traverse this search space\nto determine the frequent patterns.\nThe lattice is separated into frequent and infrequent itemsets by aborder, which is illus-\ntrated by a dashed line in Fig.4.1. All itemsets above this border are frequent, whereas those\nbelow the border are infrequent. Note that all maximal frequent itemsets are adjacent to\nthis border of itemsets. Furthermore, any valid border representing a true division between\nfrequent and infrequent itemsets will always respect the downward closure property.\n4.3 Association Rule Generation Framework\nFrequent itemsets can be used to generateassociation rules, with the use of a measure\nknown as thecondence. The condence of a ruleX  Y is the conditional probability\nthat a transaction contains the set of items Y, given that it contains the set X.T h i s",
           "Ch 4",
           5
          ],
          [
           "Chapter 4: Association Pattern Mining\nthat a transaction contains the set of items Y, given that it contains the set X.T h i s\nprobability is estimated by dividing the support of itemsetX Y with that of itemsetX.\nDenition 4.3.1 (Condence) Let X and Y be two sets of items. The condence\nconf(X Y) of the rule X Y is the conditional probability of X Y occurring in a\n98 CHAPTER 4. ASSOCIATION PATTERN MINING\ntransaction, given that the transaction containsX. Therefore, the condenceconf(X  Y)\nis dened as follows:\nconf(X  Y)= sup(X Y)\nsup(X) . (4.2)\nThe itemsetsX and Y are said to be theantecedentand theconsequentof the rule, respec-\ntively. In the case of Table4.1, the support of{Eggs,Milk } is 0.6, whereas the support\nof {Eggs,Milk,Yogurt } is 0.4. Therefore, the condence of the rule {Eggs,Milk }\n{Yogurt } is (0.4/0.6) = 2/3.\nAs in the case of support, a minimum condence thresholdminconf can be used to\ngenerate the most relevant association rules. Association rules are dened using bothsupport",
           "Ch 4",
           5
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ch 4",
         "marker": {
          "color": "#AB63FA",
          "size": {
           "bdata": "BQUFBQUFBQUFBQUFBQUF",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 4",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "ihm5iAlX2L8eYQVHzq7Uv4Hg1TWI09i/oQNdWqnB2L9r3UQ06UbhvwVL6aVjKuG/Z/Fv5Jcd3b+t4A68mnXTv0goPcszieG/ac8WcpkH4r8fbBkItpPiv28o6PM4z+K/SPioiVPP4L9ubR3CIbzgv7/Ct3e9+tu/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "n8mBmC3nw7+JEZS8cwKSv2sQQs9nyri/kMWQPr2gvL/4RKC4le6sv2/oKa7TTLq/Qi8wQ6B2ub/4xrQm8Aenv93CraQaFLC/1vMrS6KJfL9iv2DnwpGJP/qqizLPZrW/M8B/wqMuoT/tWr75gAOoP5t0Uejegow/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "aCSNDOYQcL9kwE0w3YSvPxD0FVkdhXO/3QIXwp+Kwz8mP1LWiDmqPw//kSWqiaM/4pIKlABauT9EtqXVx4HEP6WJhUDKXro/2oG/sLGSsj81A50/YjiXP20gZV6RfKs/R1KbhhAWqj8Tkzw+PVWQPw0idPjWHaI/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\n5.2. PATTERN SUMMARIZATION 137\nAlthough all the itemsets can be derived from the maximal itemsets with the subsetting\napproach, their support values cannot be derived. Therefore, maximal itemsets are lossy\nbecause they do not retain information about the support values. To provide a lossless\nrepresentation in terms of the support values, the notion ofclosed itemset miningis used.\nThis concept will be discussed in the next section.\nA trivial way to nd all the maximal itemsets would be to use any frequent itemset\nmining algorithm to ndall itemsets. Then, only the maximal ones can be retained in a\npostprocessing phase by examining itemsets in decreasing order of length, and removing\nproper subsets. This process is continued until all itemsets have either been examined or\nremoved. The itemsets that have not been removed at termination are the maximal ones.\nHowever, this approach is an inecient solution. When the itemsets are very long, the",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nHowever, this approach is an inecient solution. When the itemsets are very long, the\nnumber of maximal frequent itemsets may be orders of magnitude smaller than the number\nof frequent itemsets. In such cases, it may make sense to design algorithms that can directly\nprune parts of the search space of patterns during frequent itemset discovery. Most of the\ntree-enumeration methods can be modied with the concept oflookaheads to prune the\nsearch space of patterns. This notion is discussed in the previous chapter in the context of\nthe DepthProjectalgorithm.\nAlthough the notion of lookaheads is described in the Chap.4, it is repeated here for\ncompleteness. LetP be a frequent pattern in the enumeration tree of itemsets, andF(P)\nrepresent the set of candidate extensions ofP in the enumeration tree. Then, ifP F(P)\nis a subset of a frequent pattern that has already been found, then it implies that the\nentire enumeration tree rooted atP is frequent and can, therefore, be removed from further",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nentire enumeration tree rooted atP is frequent and can, therefore, be removed from further\nconsideration. In the event that the subtree is not pruned, the candidate extensions ofP\nneed to be counted. During counting, the support ofP F(P) is counted at the same\ntime that the supports of single-item candidate extensions ofP are counted. IfP F(P)\nis frequent then the subtree rooted atP can be pruned as well. The former kind ofsubset-\nbased pruning approach is particularly eective with depth-rst methods. This is because\nmaximal patterns are found much earlier with a depth-rst strategy than with a breadth-\nrst strategy. For a maximal pattern of lengthk, the depth-rst approach discovers it after\nexploring only (k1) of its prexes, rather than the 2\nk possibilities. This maximal pattern\nthen becomes available for subset-based pruning. The remaining subtrees containing subsets\nof P F(P) are then pruned. The superior lookahead-pruning of depth-rst methods was",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nof P F(P) are then pruned. The superior lookahead-pruning of depth-rst methods was\nrst noted in the context of theDepthProjectalgorithm.\nThe pruning approach provides a smaller set of patterns that includes all maximal\npatterns but may also include some nonmaximal patterns despite the pruning. Therefore,\ntheapproachdiscussedabovemaybeappliedtoremovethesenonmaximalpatterns.Referto\nthe bibliographic notes for pointers to various maximal frequent pattern mining algorithms.\n5.2.2 Closed Patterns\nA simple denition of a closed pattern, or closed itemset, is as follows:\nDenition 5.2.2 (Closed Itemsets) An itemsetX is closed, if none of its supersets have\nexactly the same support count asX.\nClosed frequent pattern mining algorithms require itemsets to be closed in addition to being\nfrequent. So why are closed itemsets important? Consider a closed itemsetX, and the set\nS(X) of itemsets which are subsets ofX, and which have the same support asX.T h eo n l y",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nS(X) of itemsets which are subsets ofX, and which have the same support asX.T h eo n l y\nitemset fromS(X) that will be returned by a closed frequent itemset mining algorithm, will\n138 CHAPTER 5. ASSOCIATION PATTERN MINING: ADVANCED CONCEPTS\nbe X. The itemsets contained inS(X) may be referred to as theequi-supportsubsets ofX.\nAn important observation is as follows:\nObservation 5.2.1 Let X b eac l o s e di t e m s e t ,a n dS(X) be its equi-support subsets. For\nany itemset Y S(X), the set of transactions T (Y) containing Y is exactly the same.\nFurthermore, there is no itemsetZ outside S(X) such that the set of transactions inT (Z)\nis the same asT (X).\nThis observation follows from the downward closed property of frequent itemsets. For any\nproper subsetY of X,t h es e to ft r a n s a c t i o n sT (Y)i sa l w a y sas u p e r s e to fT(X). However,\nif the support values ofX and Y are the same, thenT (X)a n dT(Y)a r et h es a m e ,a sw e l l .",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nif the support values ofX and Y are the same, thenT (X)a n dT(Y)a r et h es a m e ,a sw e l l .\nFurthermore, if any itemsetZ S(X) yields T (Z)= T (X), then the support ofZ X\nmust be the same as that ofX. BecauseZ is not a subsetof X, Z X m u s tb eap r o p e r\nsuperset ofX. This would lead to a contradiction with the assumption thatX is closed.\nIt is important to understand that the itemsetX encodes information about all the\nnonredundant counting information needed with respect to any itemset inS(X). Every\nitemset in S(X) describes the same set of transactions, and therefore, it suces to keep\nthe single representative itemset.The maximal itemsetX from S(X) is retained. It should\nbe pointed out that Denition5.2.2 is a simplication of a more formal denition that is\nbased on the use of a set-closure operator. The formal denition with the use of a set-\nclosure operator is directly based on Observation5.2.1 (which was derived here from the",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nclosure operator is directly based on Observation5.2.1 (which was derived here from the\nsimplied denition). The informal approach used by this chapter is designed for better\nunderstanding. The frequent closed itemset mining problem is dened below.\nDenition 5.2.3 (Closed Frequent Itemsets)An itemsetX i sac l o s e df r e q u e n ti t e m -\nset at minimum supportminsup,i fi ti sb o t hc l o s e da n df r e q u e n t .\nThe set of closed itemsets can be discovered in two ways:\n1. The set of frequent itemsets at any given minimum support level may be determined,\nand the closed frequent itemsets can be derived from this set.\n2. Algorithms can be designed to directly nd the closed frequent patterns during the\nprocess of frequent pattern discovery.\nWhile the second class of algorithms is beyond the scope of this book, a brief description\nof the rst approach for nding all the closed itemsets will be provided here. The reader is",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nof the rst approach for nding all the closed itemsets will be provided here. The reader is\nreferred to the bibliographic notes for algorithms of the second type.\nA simple approach for nding frequent closed itemsets is to rst partition all the frequent\nitemsets into equi-support groups. The maximal itemsets from each equi-support group may\nbe reported. Consider a set of frequent patternsF, from which the closed frequent patterns\nneed to be determined. The frequent patterns inF are processed in increasing order of\nsupport and either ruled in or ruled out, depending on whether or not they are closed.\nNote that an increasing support ordering also ensures that closed patterns are encountered\nearlierthantheirredundantsubsets.Initially,allpatternsareunmarked.Whenanunmarked\npattern X F is processed (based on the increasing support order selection), it is added\nto the frequent closed setCF. The proper subsets ofX with the same support cannot be",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nto the frequent closed setCF. The proper subsets ofX with the same support cannot be\nclosed. Therefore, all the proper subsets ofX with the same support are marked. To achieve\nthis goal, the subset of the itemset lattice representingF can be traversed in depth-rst or\nbreadth-rst order starting atX, and exploring subsets ofX. Itemsets that are subsets ofX\nare marked when they have the same support asX. The traversal process backtracks when\nan itemset is reached with strictly larger support, or the itemset has already been marked\n5.2. PATTERN SUMMARIZATION 139\nby the current or a previous traversal. After the traversal is complete, the next unmarked\nnode is selected for further exploration and added toCF. The entire process of marking\nnodes is repeated, starting from the pattern newly added toCF. At the end of the process,\nthe itemsets inCF represent the frequent closed patterns.\n5.2.3 Approximate Frequent Patterns",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nthe itemsets inCF represent the frequent closed patterns.\n5.2.3 Approximate Frequent Patterns\nApproximate frequent pattern mining schemes are almost always lossy schemes because they\ndo not retain all the information about the itemsets. The approximation of the patterns\nmay be performed in one of the following two ways:\n1. Description in terms of transactions:The closure property provides a lossless descrip-\ntion of the itemsets in terms of their membership in transactions. A generalization of\nthis idea is to allow almost closures, where the closure property is not exactly sat-\nised but is approximately specied. Thus, a play is allowed in the support values\nof the closure denition.\n2. Description in terms of itemsets themselves:In this case, the frequent itemsets are\nclustered, and representatives can be drawn from each cluster to provide a concise\nsummary. In this case, the play is allowed in terms of the distances between the\nrepresentatives and remaining itemsets.",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nrepresentatives and remaining itemsets.\nThese two types of descriptions yield dierent insights. One is dened in terms oftransaction\nmembership, whereas the other is dened in terms of thestructure of the itemset.N o t et h a t\namong the subsets of a 10-itemsetX, a 9-itemset may have a much higher support, but a\n1-itemset may have exactly the same support asX. In the rst denition, the 10-itemset\nand 1-itemset are almost redundant with respect to each other in terms of transaction\nmembership. In the second denition, the 10-itemset and 9-itemset are almost redundant\nwithrespecttoeachotherintermsofitemset structure.Thefollowing sectionswillintroduce\nmethods for discovering each of these kinds of itemsets.\n5.2.3.1 Approximation in Terms of Transactions\nThe closure property describes itemsets in terms of transactions, and the equivalence of dif-\nferent itemsets with this criterion. The notion of approximate closures is a generalization",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nferent itemsets with this criterion. The notion of approximate closures is a generalization\nof this criterion. There are multiple ways to dene approximate closure, and a simpler\ndenition is introduced here for ease in understanding.\nIn the earlier case of exact closures, one chooses the maximal supersets at a particu-\nlar support value. In approximate closures, one does not necessarily choose the maximal\nsupersets at a particular support value but allows a play, within a range of supports.\nTherefore, all frequent itemsetsF can be segmented into a disjoint set ofk almost equi-\nsupport groups F\n1 ... Fk, such that for any pair of itemsetsX,Y within any groupFi,\nthe value of|sup(X) sup(Y)| is at most. From each group,Fi, only the maximal fre-\nquent representatives are reported. Clearly, whenis chosen to be 0, this is exactly the set\nof closed itemsets. If desired, the exact error value obtained by removing individual items",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nof closed itemsets. If desired, the exact error value obtained by removing individual items\nfrom approximately closed itemsets is also stored. There is, of course, still some uncertainty\nin support values because the support values of itemsets obtained by removingtwo items\ncannot beexactly inferred from this additional data.\nNote that the almost equi-support groups may be constructed in many dierent ways\nwhen >0. This is because the ranges of the almost equi-support groups need not exactly\n140 CHAPTER 5. ASSOCIATION PATTERN MINING: ADVANCED CONCEPTS\nbe but can be less than. Of course, a greedy way of choosing the ranges is to always\npick the itemset with the lowest support and addto it to pick the upper end of the range.\nThis process is repeated to construct all the ranges. Then, the frequent closed itemsets can\nbe extracted on the basis of these ranges.\nThe algorithm for nding frequent almost closed itemsets is very similar to that of",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nThe algorithm for nding frequent almost closed itemsets is very similar to that of\nnding frequent closed itemsets. As in the previous case, one can partition the frequent\nitemsets into almost equi-support groups, and determine the maximal ones among them. A\ntraversal algorithm in terms of the graph lattice is as follows.\nThe rst step is to decide the dierent ranges of support for the almost equi-support\ngroups. The itemsets inF are processed groupwise in increasing order of support ranges\nfor the almost equi-support groups. Within a group, unmarked itemsets are processed in\nincreasing order of support. When these nodes are examined they are added to the almost\nclosed setAC. When a patternX F is examined, all its proper subsets within the same\ngroup are marked, unless they have already been marked. To achieve this goal, the subset\nof the itemset lattice representingF can be traversed in the same way as discussed in the",
           "Ch 5",
           5
          ],
          [
           "Chapter 5: Association Pattern Mining: Advanced Concepts\nof the itemset lattice representingF can be traversed in the same way as discussed in the\nprevious case of (exactly) closed sets. This process is repeated with the next unmarked\nnode. At the end of the process, the setAC contains the frequent almost closed patterns.\nA variety of other ways of dening almost closed itemsets are available in the literature.\nThe bibliographic notes contain pointers to these methods.\n5.2.3.2 Approximation in Terms of Itemsets\nThe approximation in terms of itemsets can also be dened in many dierent ways and\nis closely related to clustering. Conceptually, the goal is to create clusters from the set of\nfrequent itemsetscalF, and pick representativesJ = J\n1 ...J k from the clusters. Because\nclusters are always dened with respect to a distance functionDist(X,Y ) between itemsets\nX and Y, the notion of-approximate sets is also based on a distance function.\nDenition 5.2.4 (-Approximate Sets) The set of representativesJ = {J1 ...J k} is",
           "Ch 5",
           5
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ch 5",
         "marker": {
          "color": "#FFA15A",
          "size": {
           "bdata": "BQUFBQUFBQUFBQUFBQUF",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 5",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "4zxwZsGp27+90EbnfyPZv+4DOXHd7tW/XQ3ZbAHS37/d6+YrZ9fhv+bRI2vK6N+/U1SD7Rca4r+i5URXG87hv6U5le3cQOG/Gszng1Ss278ldk2tZZTcv74JwGj1Q9y/R9OghJM+3L/DXKpkxCXfv5jdbFxSSNe/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "oo+Q/40dpL9wjoRc1ql6v2svR8YD7Kw/jsvWvrZKnj/2MjmPeZ+pP8G9x8O98bE/cb7WiA+MmL/3JeGvX0ymP8Z6xEM3BbE/k/w6zV/Jtj+Qirv8Bfu1P74JqUgVy8Q/M9bgy5teqz8dVPXWQZK2P75a3PXztMo/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "C8DSfq65lT9x/NeObHe1v97OUDTm3cC/6h2CBA47qb+beCGPgkKuP930jz/Bk30/6KYPCegKpL9MGtm4+8qov2O163CJV62/MLbz+uLuej+VOwfglL6qP8ogy5pScqe/GkBkLuU8br9rVCI3z4p9v0nf6ngBN3W/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "Chapter 6: Cluster Analysis\n6.2. FEATURE SELECTION FOR CLUSTERING 155\n2. Wrapper models:In this case, a clustering algorithm is used to evaluate the quality\nof a subset of features. This is then used to rene the subset of features on which\nthe clustering is performed. This is a naturally iterative approach in which a good\nchoice of features depends on the clusters and vice versa. The features selected will\ntypically be at least somewhat dependent on the particular methodology used for\nclustering. Although this may seem like a disadvantage, the fact is that dierent\nclustering methods may work better with dierent sets of features. Therefore, this\nmethodology can also optimize the feature selection to the specic clustering tech-\nnique. On the other hand, the inherent informativeness of the specic features may\nsometimes not be reected by this approach due to the impact of the specic clustering\nmethodology.\nA major distinction between lter and wrapper models is that the former can be performed",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nA major distinction between lter and wrapper models is that the former can be performed\npurely as a preprocessing phase, whereas the latter is integrated directly into the clus-\ntering process. In the following sections, a number of lter and wrapper models will be\ndiscussed.\n6.2.1 Filter Models\nIn lter models, a specic criterion is used to evaluate the impact of specic features, or\nsubsets of features, on the clustering tendency of the data set. The following will introduce\nmany of the commonly used criteria.\n6.2.1.1 Term Strength\nTerm strength is suitable for sparse domains such as text data. In such domains, it is more\nmeaningful to talk about presence or absence of nonzero values on the attributes (words),\nrather than distances. Furthermore, it is more meaningful to use similarity functions rather\nthan distance functions. In this approach, pairs of documents are sampled, but a random\nordering is imposed between the pair. The term strength is dened as the fraction of similar",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nordering is imposed between the pair. The term strength is dened as the fraction of similar\ndocument pairs (with similarity greater than ), in which the term occurs in both the\ndocuments, conditional on the fact that it appears in the rst. In other words, for any term\nt, and document pair (\nX,Y) that have been deemed to be suciently similar, the term\nstrength is dened as follows:\nTerm Strength =P(t Y|t X). (6.1)\nIf desired, term strength can also be generalized to multidimensional data by discretizing the\nquantitative attributes into binary values. Other analogous measures use the correlations\nbetween the overall distances and attribute-wise distances to model relevance.\n6.2.1.2 Predictive Attribute Dependence\nThe intuitive motivation of this measure is that correlated features will always result in\nbetter clusters than uncorrelated features. When an attribute is relevant, other attributes",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nbetter clusters than uncorrelated features. When an attribute is relevant, other attributes\ncan be used to predict the value of this attribute. A classication (or regression modeling)\nalgorithm can be used to evaluate this predictiveness. If the attribute is numeric, then a\nregression modeling algorithm is used. Otherwise, a classication algorithm is used. The\noverall approach for quantifying the relevance of an attributei is as follows:\n156 CHAPTER 6. CLUSTER ANALYSIS\nFigure 6.1: Impact of clustered data on distance distribution entropy\n1. Use a classication algorithm on all attributes, except attributei, to predict the value\nof attributei, while treating it as an articial class variable.\n2. Report the classication accuracy as the relevance of attributei.\nAny reasonable classication algorithm can be used, although a nearest neighbor classier\nis desirable because of its natural connections with similarity computation and clustering.",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nis desirable because of its natural connections with similarity computation and clustering.\nClassication algorithms are discussed in Chap.10.\n6.2.1.3 Entropy\nThe basic idea behind these methods is that highly clustered data reects some of its\nclustering characteristics on the underlying distance distributions. To illustrate this point,\ntwo dierent data distributions are illustrated in Figures6.1a and b, respectively. The rst\nplot depicts uniformly distributed data, whereas the second one depicts data with two\nclusters. In Figures6.1c and d, the distribution of the pairwise point-to-point distances is\nillustrated for the two cases. It is evident that the distance distribution for uniform data is\narranged in the form of a bell curve, whereas that for clustered data has two dierent peaks\ncorresponding to the intercluster distributions and intracluster distributions, respectively.\nThe number of such peaks will typically increase with the number of clusters. The goal",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nThe number of such peaks will typically increase with the number of clusters. The goal\nof entropy-based measures is to quantify the shape of this distance distributionon a\ngiven subset of features, and then pick the subset where the distribution shows behavior\nthat is more similar to the case of Fig.6.1b. Therefore, such algorithms typically require\n6.2. FEATURE SELECTION FOR CLUSTERING 157\na systematic way to search for the appropriate combination of features, in addition to\nquantifying the distance-based entropy. So how can the distance-based entropy be quantied\non a particular subset of attributes?\nA natural way of quantifying the entropy is to directly use the probability distribution\non the data points and quantify the entropy using these values. Consider ak-dimensional\nsubset of features. The rst step is to discretize the data into a set of multidimensional grid\nregions using grid regions for each dimension. This results inm = \nk grid ranges that",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nregions using grid regions for each dimension. This results inm = \nk grid ranges that\nare indexed from 1 throughm.T h ev a l u eo fmis approximately the same across all the\nevaluated feature subsets by selecting= m1/k.I fpi is the fraction of data points in grid\nregion i, then the probability-based entropyE is dened as follows:\nE = \nm\ni=1\n[pilog(pi)+(1 pi)log(1pi)]. (6.2)\nA uniform distribution with poor clustering behavior has high entropy, whereas clustered\ndata has lower entropy. Therefore, the entropy measure provides feedback about the clus-\ntering quality of a subset of features.\nAlthough the aforementioned quantication can be used directly, the probability density\npi of grid regioniis sometimes hard to accurately estimate from high-dimensional data. This\nis because the grid regions are multidimensional, and they become increasingly sparse in\nhigh dimensionality. It is also hard to x the number of grid regionsm over feature subsets",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nhigh dimensionality. It is also hard to x the number of grid regionsm over feature subsets\nof varying dimensionalityk because the value of= m\n1/k is rounded up to an integer\nvalue. Therefore, an alternative is to compute the entropy on the 1-dimensional point-to-\npoint distance distribution on a sample of the data. This is the same as the distributions\ns h o w ni nF i g .6.1.T h ev a l u eo fpi then represents the fraction ofdistances in the ith 1-\ndimensional discretized range. Although this approach does not fully address the challenges\nof high dimensionality, it is usually a better option for data of modest dimensionality. For\nexample, if the entropy is computed on the histograms in Figs.6.1c and d, then this will\ndistinguish between the two distributions well. A heuristic approximation on the basis of\nthe raw distances is also often used. Refer to the bibliographic notes.\nTo determine the subset of features, for which the entropyE is minimized, a variety of",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nTo determine the subset of features, for which the entropyE is minimized, a variety of\nsearch strategies are used. For example, starting from the full set of features, a simple greedy\napproach may be used to drop the feature that leads to the greatest reduction in the entropy.\nFeatures are repeatedly dropped greedily until the incremental reduction is not signicant,\nor the entropy increases. Some enhancements of this basic approach, both in terms of the\nquantication measure and the search strategy, are discussed in the bibliographic section.\n6.2.1.4 Hopkins Statistic\nTheHopkinsstatisticisoftenusedtomeasuretheclusteringtendencyofadataset,although\nit can also be applied to a particular subset of attributes. The resulting measure can then be\nused in conjunction with a feature search algorithm, such as the greedy method discussed\nin the previous subsection.\nLet D be the data set whose clustering tendency needs to be evaluated. A sampleS of r",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nLet D be the data set whose clustering tendency needs to be evaluated. A sampleS of r\nsynthetic data points is randomly generated in the domain of the data space. At the same\ntime, a sampleR of r data points is selected fromD.L e t\n1 ...r be the distances of the\ndata points in the sampleR D to their nearest neighbors within the original databaseD.\nSimilarly, let1 ...r be the distances of the data points in the synthetic sampleS to their\n158 CHAPTER 6. CLUSTER ANALYSIS\nnearest neighbors withinD. Then, the Hopkins statisticH is dened as follows:\nH =\nr\ni=1 ir\ni=1\n(i +i). (6.3)\nThe Hopkins statistic will be in the range (0,1). Uniformly distributed data will have a\nHopkins statistic of 0.5 because the values ofi and i will be similar. On the other hand,\nthe values ofi will typically be much lower thani for the clustered data. This will result\nin a value of the Hopkins statistic that is closer to 1. Therefore, a high value of the Hopkins",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nin a value of the Hopkins statistic that is closer to 1. Therefore, a high value of the Hopkins\nstatistic H is indicative of highly clustered data points.\nOne observation is that the approach uses random sampling, and therefore the measure\nwill vary across dierent random samples. If desired, the random sampling can be repeated\nover multiple trials. A statistical tail condence test can be employed to determine the\nlevel of condence at which the Hopkins statistic is greater than 0.5. For feature selection,\nthe average value of the statistic over multiple trials can be used. This statistic can be\nused to evaluate the quality of any particular subset of attributes to evaluate the clustering\ntendency of that subset. This criterion can be used in conjunction with a greedy approach\nto discover the relevant subset of features. The greedy approach is similar to that discussed\nin the case of the distance-based entropy method.\n6.2.2 Wrapper Models",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nin the case of the distance-based entropy method.\n6.2.2 Wrapper Models\nWrapper models use aninternal cluster validity criterionin conjunction with a clustering\nalgorithm that is applied to an appropriate subset of features. Cluster validity criteria are\nused to evaluate the quality of clustering and are discussed in detail in Sect.6.9.T h ei d e a\nis to use a clustering algorithm with a subset of features, and then evaluate the quality\nof this clustering with a cluster validity criterion. Therefore, the search space of dierent\nsubsets of features need to be explored to determine the optimum combination of features.\nAs the search space of subsets of features is exponentially related to the dimensionality,\na greedy algorithm may be used to successively drop features that result in the greatest\nimprovement of the cluster validity criterion. The major drawback of this approach is that\nit is sensitive to the choice of the validity criterion. As you will learn in this chapter, cluster",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nit is sensitive to the choice of the validity criterion. As you will learn in this chapter, cluster\nvalidity criteria are far from perfect. Furthermore, the approach can be computationally\nexpensive.\nAnother simpler methodology is to select individual features with a feature selection cri-\nterion that is borrowed from that used in classication algorithms. In this case, the features\nare evaluated individually, rather than collectively, as a subset. The clustering approach\narticially creates a set of labelsL, corresponding to the cluster identiers of the individual\ndata points. A feature selection criterion may be borrowed from the classication literature\nwith the use of the labels inL. This criterion is used to identify the most discriminative\nfeatures:\n1. Use a clustering algorithm on the current subset of selected featuresF, in order to x\ncluster labelsL for the data points.\n2. Use any supervised criterion to quantify the quality of the individual features with",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\n2. Use any supervised criterion to quantify the quality of the individual features with\nrespect to labelsL. Select the top-kfeatures on the basis of this quantication.\nThere is considerable exibility in the aforementioned framework, where dierent kinds of\nclustering algorithms and feature selection criteria are used in each of the aforementioned\nsteps. A variety of supervised criteria can be used, such as theclass-based entropyor the\n6.3. REPRESENTATIVE-BASED ALGORITHMS 159\nAlgorithm GenericRepresentative(Database: D,N u m b e ro fR e p r e s e n t a t i v e s :k)\nbegin\nInitialize representative setS;\nrepeat\nCreate clusters (C1 ... Ck)b ya s s i g n i n ge a c h\npoint inD to closest representative inS\nusing the distance functionDist(,);\nRecreate setS by determining one representativeYj for\neach Cj that minimizes \nXiCj\nDist(Xi,Yj);\nuntil convergence;\nreturn (C1 ... Ck);\nend\nFigure 6.2: Generic representative algorithm with unspecied distance function",
           "Ch 6",
           5
          ],
          [
           "Chapter 6: Cluster Analysis\nend\nFigure 6.2: Generic representative algorithm with unspecied distance function\nFisher score (cf. Sect. 10.2 of Chap. 10). The Fisher score, discussed in Sect.10.2.1.3 of\nChap. 10, measures the ratio of the intercluster variance to the intracluster variance on any\nparticular attribute. Furthermore, it is possible to apply this two-step procedure iteratively.\nHowever, some modications to the rst step are required. Instead of selecting the top-k\nfeatures, the weights of the top-k features are set to 1, and the remainder are set to<1.\nHere, is a user-specied parameter. In the nal step, the top-k features are selected.\nWrapper models are often combined with lter models to createhybrid modelsfor better\neciency.Inthiscase,candidatefeaturesubsetsareconstructedwiththeuseofltermodels.\nThen, the quality of each candidate feature subset is evaluated with a clustering algorithm.\nThe evaluation can be performed either with a cluster validity criterion or with the use of",
           "Ch 6",
           5
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "Ch 6",
         "marker": {
          "color": "#19D3F3",
          "size": {
           "bdata": "BQUFBQUFBQUFBQUFBQUF",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Ch 6",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "rtIXgCp6yj+xon6Z7W3JP9aeAEIlA78/nl5OcOJfyT/XloCOkVrOP65n1Sn/E8w/NO/HPMspyj9VQDWVuovPP737GPj6J7c/stVmax70xT9sg0xVBQDAPyYNOeNRpcY/jzclTUg8yj9YEtH6lUPFP895ks8Ls8g/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "4Zb14vbm0T+0HNWDUjzSP/D1YvdMl9c/Ku0zFD0d1D872sEYRBPTP386PfQOUNo/Wz28yZoo1z/muPtUPXnVP60Es6gdWNI/tqcugWUF1D+ZIgtaP6LVP+ceGxn39Ng/bbaNSgYX1T+Ybk7oqq7XP5JpxANlutg/",
          "dtype": "f8"
         },
         "z": {
          "bdata": "Iy7qb+ZG4b91HRioQrrQv8Tbt0ZJDr6/7soM5dWHzb+O7490C1eIv4nNjJTOING/9PMT5VcJ0L/DCid5WOLDvwTyIsHM2tq/I4c+tMPhaL/Z0ymCVnvZvzYhMJLrMt6/whjMdFmv3r9LvNLJmnLVv89nRqWtFdm/",
          "dtype": "f8"
         }
        },
        {
         "customdata": [
          [
           "What is Lp norm?",
           "USER QUERY",
           15
          ]
         ],
         "hovertemplate": "Category=%{customdata[1]}<extra></extra>",
         "legendgroup": "USER QUERY",
         "marker": {
          "color": "#FF6692",
          "size": {
           "bdata": "Dw==",
           "dtype": "i1"
          },
          "sizemode": "area",
          "sizeref": 0.06666666666666667,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "USER QUERY",
         "scene": "scene",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "1JAqhiu4uz8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "zDphf/HtxD8=",
          "dtype": "f8"
         },
         "z": {
          "bdata": "x/QRdO56zz8=",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "legend": {
         "itemsizing": "constant",
         "title": {
          "text": "Category"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 40
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "PC1"
          }
         },
         "yaxis": {
          "title": {
           "text": "PC2"
          }
         },
         "zaxis": {
          "title": {
           "text": "PC3"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "User question and book chapters"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: 'What is Lp norm?'\n",
      "\n",
      "--- Top 3 Most Similar Textbook Chunks (Retrieved Context) ---\n",
      "\n",
      "[Match 1] Similarity Score: 0.5758\n",
      "Chapter: Ch 3\n",
      "Snippet: Chapter 3: Similarity and Distances\n",
      "3.2 Multidimensional Data\n",
      "Although multidimensional data are the simplest form of data, there is signicant diversity\n",
      "indistancefunctiondesignacrossdierentattributetypessuchascategoricalorquantitative\n",
      "data. This section will therefore study each of these types se...\n",
      "\n",
      "[Match 2] Similarity Score: 0.5435\n",
      "Chapter: Ch 3\n",
      "Snippet: Chapter 3: Similarity and Distances\n",
      "3.2. MULTIDIMENSIONAL DATA 67\n",
      "Figure 3.2: Impact ofp on contrast\n",
      "subset selection during preprocessing, because the relevance of features islocallydetermined\n",
      "by the pair of objects that are being considered. Globally, all features may be relevant.\n",
      "Whenmanyfeatures...\n",
      "\n",
      "[Match 3] Similarity Score: 0.4818\n",
      "Chapter: Ch 3\n",
      "Snippet: Chapter 3: Similarity and Distances\n",
      "particular application. For example, for a credit-scoring application, an attribute such as\n",
      "salary is much more relevant to the design of the distance function than an attribute such\n",
      "as gender, though both may have some impact. In such cases, the analyst may choos...\n"
     ]
    }
   ],
   "source": [
    "user_query_rag = \"What is Lp norm?\"\n",
    "\n",
    "query_embedding_rag = embedding_model.embed_documents([user_query_rag])[0]\n",
    "query_vector_rag = np.array(query_embedding_rag)\n",
    "\n",
    "reduced_query_rag = pca.transform([query_vector_rag])\n",
    "\n",
    "all_points = np.vstack([reduced_book, reduced_query_rag])\n",
    "all_texts = chapter_chunks + [user_query_rag]\n",
    "all_categories = chapter_labels + [\"USER QUERY\"]\n",
    "all_sizes = [5] * len(chapter_chunks) + [15]\n",
    "\n",
    "df_rag = pd.DataFrame({\n",
    "    'PC1': all_points[:, 0],\n",
    "    'PC2': all_points[:, 1],\n",
    "    'PC3': all_points[:, 2],\n",
    "    'Text': all_texts,\n",
    "    'Category': all_categories,\n",
    "    'Size': all_sizes\n",
    "})\n",
    "\n",
    "rag_fig = px.scatter_3d(\n",
    "    df_rag,\n",
    "    x='PC1', y='PC2', z='PC3',\n",
    "    color='Category',\n",
    "    size='Size',\n",
    "    size_max=15,\n",
    "    hover_data={'PC1': False, 'PC2': False, 'PC3': False, 'Text': False, 'Category': True, 'Size': False},\n",
    "    title=\"User question and book chapters\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Plotly\n",
    ")\n",
    "rag_fig.update_layout(margin=dict(l=0, r=0, b=0, t=40))\n",
    "rag_fig.show()\n",
    "\n",
    "scores = [cosine_similarity(query_vector_rag, chunk_vector) for chunk_vector in book_embeddings_array]\n",
    "\n",
    "top_indices = np.argsort(scores)[-3:][::-1]\n",
    "\n",
    "context = \"\"\n",
    "print(f\"User Query: '{user_query_rag}'\")\n",
    "print(\"\\n--- Top 3 Most Similar Textbook Chunks (Retrieved Context) ---\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    context += chapter_chunks[idx]\n",
    "    context += \"\\n\"\n",
    "    print(f\"\\n[Match {i+1}] Similarity Score: {scores[idx]:.4f}\")\n",
    "    print(f\"Chapter: {chapter_labels[idx]}\")\n",
    "    print(f\"Snippet: {chapter_chunks[idx][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b65a5",
   "metadata": {},
   "source": [
    "As we can see our question about Lp norms show up in the vector space closest to chunks from Chapter 3: Similarity and Distances. \n",
    "\n",
    "We pass these <u>retrieved</u> chunks as additional context to an LLM <u>augmenting</u> its limited knowledge while <u>generating</u> an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util.env_check import get_llm_model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "llm = get_llm_model()\n",
    "\n",
    "system_message = (\n",
    "    \"You are an expert Data Mining Tutor helping a student study from Charu C. Aggarwal's 'Data Mining: The Textbook'.\\n\\n\"\n",
    "    \n",
    "    \"Your goal is to provide clear, educational responses structured into two distinct parts: \\n\"\n",
    "    \"1. Theoretical Explanation \\n\"\n",
    "    \"2. Python Code Implementation \\n\\n\"\n",
    "    \n",
    "    \"### Guidelines:\\n\"\n",
    "    \"- STRICT CONTEXT FOR THEORY: You MUST base your theoretical explanation ONLY on the provided context snippets. Do not invent theories, formulas, or include concepts not found in the text. \"\n",
    "    \"If the context does not contain enough information to answer the question, state clearly: 'The provided text does not contain enough information to answer this.'\\n\"\n",
    "    \"- EXTERNAL KNOWLEDGE FOR CODE: Because the textbook focuses on mathematical theory, you are explicitly allowed and encouraged to use your general programming knowledge to write Python code (e.g., using pandas, numpy, scikit-learn). The code must accurately practically demonstrate the specific theoretical concepts discussed in the context.\\n\"\n",
    "    \"- MATH FORMATTING: When writing mathematical formulas, you MUST use LaTeX notation:\\n\"\n",
    "    \"  - Use double dollar signs for standalone equations (e.g., $$E=mc^2$$).\\n\"\n",
    "    \"  - Use single dollar signs for inline math (e.g., $x^2$).\\n\"\n",
    "    \"  - Do not use brackets like \\\\[ \\\\] or \\\\( \\\\) for math.\\n\"\n",
    "    \"- TONE AND STRUCTURE: Be encouraging, clear, and pedagogical. Use Markdown formatting, clear headings, and bullet points to make your explanations scannable and easy to digest.\\n\\n\"\n",
    "    \n",
    "    \"Here is the context:\\n\"\n",
    "    f\"{context}\"\n",
    ")\n",
    "\n",
    "messages = [SystemMessage(content=system_message)]\n",
    "    \n",
    "messages.append(HumanMessage(content=user_query_rag))\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a206b227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Theoretical Explanation\n",
       "\n",
       "The **Lp-norm** is a distance function commonly used to measure the difference between two data points in a multidimensional space. It is particularly useful for quantitative data. The Lp-norm between two data points \\( X = (x_1, \\ldots, x_d) \\) and \\( Y = (y_1, \\ldots, y_d) \\) is defined as:\n",
       "\n",
       "$$\n",
       "\\text{Dist}(X, Y) = \\left( \\sum_{i=1}^{d} |x_i - y_i|^p \\right)^{1/p}\n",
       "$$\n",
       "\n",
       "- **Parameters**:\n",
       "  - The value of \\( p \\) determines the type of distance:\n",
       "    - \\( p = 2 \\): The Euclidean distance, which is the straight-line distance between two points.\n",
       "    - \\( p = 1 \\): The Manhattan distance, which is like the distance one would travel on a grid of streets (like those in Manhattan).\n",
       "  - \\( d \\) is the dimensionality of the data.\n",
       "\n",
       "- **Usage**:\n",
       "  - The Lp-norm is versatile and can adapt to different types of data by varying \\( p \\). It is especially important in spatial applications where distances have clear physical interpretations.\n",
       "\n",
       "### Python Code Implementation\n",
       "\n",
       "Below is a Python implementation demonstrating how to calculate the Lp-norm between two points using the `numpy` library:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "\n",
       "def lp_norm(X, Y, p=2):\n",
       "    \"\"\"\n",
       "    Calculate the Lp-norm distance between two points X and Y.\n",
       "\n",
       "    Parameters:\n",
       "    X (array-like): The first data point.\n",
       "    Y (array-like): The second data point.\n",
       "    p (int): The order of the norm (default is 2, which is the Euclidean distance).\n",
       "\n",
       "    Returns:\n",
       "    float: The Lp-norm distance.\n",
       "    \"\"\"\n",
       "    X = np.array(X)\n",
       "    Y = np.array(Y)\n",
       "    distance = np.sum(np.abs(X - Y) ** p) ** (1 / p)\n",
       "    return distance\n",
       "\n",
       "# Example usage\n",
       "X = [1, 2, 3]\n",
       "Y = [4, 5, 6]\n",
       "\n",
       "# Calculate Euclidean distance (p=2)\n",
       "euclidean_distance = lp_norm(X, Y, p=2)\n",
       "print(\"Euclidean Distance:\", euclidean_distance)\n",
       "\n",
       "# Calculate Manhattan distance (p=1)\n",
       "manhattan_distance = lp_norm(X, Y, p=1)\n",
       "print(\"Manhattan Distance:\", manhattan_distance)\n",
       "```\n",
       "\n",
       "This code defines a function `lp_norm` that calculates the Lp-norm distance between two vectors `X` and `Y`. You can specify the order `p` to switch between different norms (such as Euclidean and Manhattan)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75e80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-tutor-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
